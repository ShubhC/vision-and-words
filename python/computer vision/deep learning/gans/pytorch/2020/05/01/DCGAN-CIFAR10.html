<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Implementing a Simple DCGAN in Pytorch | Vision and Words</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Implementing a Simple DCGAN in Pytorch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Here is a simple DCGAN implementation for generating data based on the CIFAR-10 dataset. I’ve mainly done this to try out logging and experiment tracking using Weights &amp; Biases" />
<meta property="og:description" content="Here is a simple DCGAN implementation for generating data based on the CIFAR-10 dataset. I’ve mainly done this to try out logging and experiment tracking using Weights &amp; Biases" />
<link rel="canonical" href="https://ssundar6087.github.io/vision-and-words/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html" />
<meta property="og:url" content="https://ssundar6087.github.io/vision-and-words/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html" />
<meta property="og:site_name" content="Vision and Words" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-01T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Here is a simple DCGAN implementation for generating data based on the CIFAR-10 dataset. I’ve mainly done this to try out logging and experiment tracking using Weights &amp; Biases","@type":"BlogPosting","headline":"Implementing a Simple DCGAN in Pytorch","dateModified":"2020-05-01T00:00:00-05:00","datePublished":"2020-05-01T00:00:00-05:00","url":"https://ssundar6087.github.io/vision-and-words/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://ssundar6087.github.io/vision-and-words/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/vision-and-words/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ssundar6087.github.io/vision-and-words/feed.xml" title="Vision and Words" /><link rel="shortcut icon" type="image/x-icon" href="/vision-and-words/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Implementing a Simple DCGAN in Pytorch | Vision and Words</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Implementing a Simple DCGAN in Pytorch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Here is a simple DCGAN implementation for generating data based on the CIFAR-10 dataset. I’ve mainly done this to try out logging and experiment tracking using Weights &amp; Biases" />
<meta property="og:description" content="Here is a simple DCGAN implementation for generating data based on the CIFAR-10 dataset. I’ve mainly done this to try out logging and experiment tracking using Weights &amp; Biases" />
<link rel="canonical" href="https://ssundar6087.github.io/vision-and-words/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html" />
<meta property="og:url" content="https://ssundar6087.github.io/vision-and-words/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html" />
<meta property="og:site_name" content="Vision and Words" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-01T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Here is a simple DCGAN implementation for generating data based on the CIFAR-10 dataset. I’ve mainly done this to try out logging and experiment tracking using Weights &amp; Biases","@type":"BlogPosting","headline":"Implementing a Simple DCGAN in Pytorch","dateModified":"2020-05-01T00:00:00-05:00","datePublished":"2020-05-01T00:00:00-05:00","url":"https://ssundar6087.github.io/vision-and-words/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://ssundar6087.github.io/vision-and-words/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://ssundar6087.github.io/vision-and-words/feed.xml" title="Vision and Words" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
<script type="text/javascript">
require.config({
  paths: {
    jquery: 'https://code.jquery.com/jquery-3.5.0.min',
    plotly: 'https://cdn.plot.ly/plotly-latest.min'
  },

  shim: {
    plotly: {
      deps: ['jquery'],
      exports: 'plotly'
    }
  }
});
</script>

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/vision-and-words/">Vision and Words</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/vision-and-words/about/">About Me</a><a class="page-link" href="/vision-and-words/search/">Search</a><a class="page-link" href="/vision-and-words/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Implementing a Simple DCGAN in Pytorch</h1><p class="page-description">Here is a simple DCGAN implementation for generating data based on the CIFAR-10 dataset. I've mainly done this to try out logging and experiment tracking using Weights & Biases</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-05-01T00:00:00-05:00" itemprop="datePublished">
        May 1, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      22 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/vision-and-words/categories/#Python">Python</a>
        &nbsp;
      
        <a class="category-tags-link" href="/vision-and-words/categories/#Computer Vision">Computer Vision</a>
        &nbsp;
      
        <a class="category-tags-link" href="/vision-and-words/categories/#Deep Learning">Deep Learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/vision-and-words/categories/#GANs">GANs</a>
        &nbsp;
      
        <a class="category-tags-link" href="/vision-and-words/categories/#Pytorch">Pytorch</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/ssundar6087/vision-and-words/tree/master/_notebooks/2020-05-01-DCGAN-CIFAR10.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/vision-and-words/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/ssundar6087/vision-and-words/master?filepath=_notebooks%2F2020-05-01-DCGAN-CIFAR10.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/vision-and-words/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/ssundar6087/vision-and-words/blob/master/_notebooks/2020-05-01-DCGAN-CIFAR10.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/vision-and-words/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Overview">Overview </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Basic-GAN-Loss-Function:">Basic GAN Loss Function: </a></li>
<li class="toc-entry toc-h3"><a href="#Special-Features-of-the-DCGAN:">Special Features of the DCGAN: </a></li>
<li class="toc-entry toc-h3"><a href="#Implementation-Details:">Implementation Details: </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Requirements">Requirements </a></li>
<li class="toc-entry toc-h2"><a href="#Parameters-of-Interest">Parameters of Interest </a></li>
<li class="toc-entry toc-h2"><a href="#Model-Definition">Model Definition </a></li>
<li class="toc-entry toc-h2"><a href="#Defining-the-Training-Function">Defining the Training Function </a></li>
<li class="toc-entry toc-h2"><a href="#Monitoring-the-Run">Monitoring the Run </a></li>
<li class="toc-entry toc-h2"><a href="#Loss-Curve-and-Results">Loss Curve and Results </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Loss">Loss </a></li>
<li class="toc-entry toc-h3"><a href="#Images">Images </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#References">References </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-05-01-DCGAN-CIFAR10.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Overview">
<a class="anchor" href="#Overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview<a class="anchor-link" href="#Overview"> </a>
</h2>
<p>Below is a short overview of the key features of a DCGAN</p>
<h3 id="Basic-GAN-Loss-Function:">
<a class="anchor" href="#Basic-GAN-Loss-Function:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Basic GAN Loss Function:<a class="anchor-link" href="#Basic-GAN-Loss-Function:"> </a>
</h3>
<p>$\underset{G}{\text{min}} \underset{D}{\text{max}}V(D,G) = \mathbb{E}_{x\sim p_{data}(x)}\big[logD(x)\big] + \mathbb{E}_{z\sim p_{z}(z)}\big[log(1-D(G(z)))\big]$</p>
<h3 id="Special-Features-of-the-DCGAN:">
<a class="anchor" href="#Special-Features-of-the-DCGAN:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Special Features of the DCGAN:<a class="anchor-link" href="#Special-Features-of-the-DCGAN:"> </a>
</h3>
<ul>
<li>Explicitly uses convolutional layers in the discriminator and transposed-convolutional layers in the  generator</li>
<li>Further the discriminator uses batch norm layers and <code>LeakyReLU</code> activations while the generator uses <code>ReLU</code> activations</li>
<li>The input is a latent vector drawn from a standard normal distribution and the output is a <code>3 x 32 x 32</code> RGB image</li>
<li>In this implementation, I also added in <a href="https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06">label smoothing</a>
</li>
<li>More details are in the <a href="https://arxiv.org/pdf/1511.06434.pdf">paper</a>
</li>
</ul>
<h3 id="Implementation-Details:">
<a class="anchor" href="#Implementation-Details:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementation Details:<a class="anchor-link" href="#Implementation-Details:"> </a>
</h3>
<p>I've borrowed the majority of code for this from the wonderful Pytorch tutorial <a href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html">here</a> but have made a couple of tweaks. The first of these is the structure of the generator and discriminator. Since the CIFAR-10 dataset has images of size <code>32 x 32</code>, the output size of the generator and input size of the discriminator have to be changed. Secondly, I added in label smoothing to help with the stability of the training process.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Requirements">
<a class="anchor" href="#Requirements" aria-hidden="true"><span class="octicon octicon-link"></span></a>Requirements<a class="anchor-link" href="#Requirements"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="c1">#Author: Sairam Sundaresan</span>
<span class="c1">#Version: 1.0</span>
<span class="c1">#Date May 1, 2020</span>
<span class="c1"># Preliminaries</span>
<span class="c1"># WandB – Install the W&amp;B library</span>
<span class="o">%</span><span class="k">pip</span> install wandb -q
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">random</span> <span class="c1"># to set the python random seed</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.animation</span> <span class="k">as</span> <span class="nn">animation</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.nn.parallel</span>
<span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="k">as</span> <span class="nn">cudnn</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span>
<span class="kn">import</span> <span class="nn">torchvision.utils</span> <span class="k">as</span> <span class="nn">vutils</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="c1"># Ignore excessive warnings</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">propagate</span> <span class="o">=</span> <span class="kc">False</span> 
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="c1"># Set random seed for reproducibility</span>
<span class="n">manualSeed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">manualSeed</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">manualSeed</span><span class="p">)</span>

<span class="c1"># WandB – Import the wandb library</span>
<span class="kn">import</span> <span class="nn">wandb</span>
<span class="n">wandb</span><span class="o">.</span><span class="n">login</span><span class="p">()</span>
<span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s2">"dcgan"</span><span class="p">)</span> <span class="c1"># Change the project name based on your W &amp; B account</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Parameters-of-Interest">
<a class="anchor" href="#Parameters-of-Interest" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parameters of Interest<a class="anchor-link" href="#Parameters-of-Interest"> </a>
</h2>
<p>Note that the Pytorch tutorial <a href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html">referenced below</a> is designed for the <strong>Celebrity faces</strong> dataset and produces <code>64 x 64</code> images. I've tweaked the network architecture to produce <code>32 x 32</code> images as corresponding to the <strong>CIFAR-10</strong> dataset. The parameters below reflect the same.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Number of workers for dataloader</span>
<span class="n">workers</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Batch size during training</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># Spatial size of training images. All images will be resized to this</span>
<span class="c1">#   size using a transformer.</span>
<span class="n">image_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># Number of channels in the training images. For color images this is 3</span>
<span class="n">nc</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Size of z latent vector (i.e. size of generator input)</span>
<span class="n">nz</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Size of feature maps in generator</span>
<span class="n">ngf</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Size of feature maps in discriminator</span>
<span class="n">ndf</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Number of training epochs</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">30</span>

<span class="c1"># Learning rate for optimizers</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0002</span>

<span class="c1"># Beta1 hyperparam for Adam optimizers</span>
<span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># Number of GPUs available. Use 0 for CPU mode.</span>
<span class="n">ngpu</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-Definition">
<a class="anchor" href="#Model-Definition" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model Definition<a class="anchor-link" href="#Model-Definition"> </a>
</h2>
<p>Let's define a generator and discriminator first. Weight initialization is a key factor in being able to produce a decent GAN and as per the paper, the weights are drawn from a <em>normal</em> distribution with <code>0</code> mean and a standard-deviation of <code>0.02</code>. Also note that unlike in the original pytorch tutorial, I've removed one layer from the generator (at the end) and from the discriminator (at the beginning) to accomodate the CIFAR-10 dataset.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">weights_init</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="n">classname</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">if</span> <span class="n">classname</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">'Conv'</span><span class="p">)</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">classname</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">'BatchNorm'</span><span class="p">)</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Generator</span>
<span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ngpu</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Generator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">=</span> <span class="n">ngpu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">main</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span> <span class="n">nz</span><span class="p">,</span> <span class="n">ngf</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">ngf</span> <span class="o">*</span> <span class="mi">8</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">ngf</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">ngf</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">ngf</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span> <span class="n">ngf</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">ngf</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">ngf</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span> <span class="n">ngf</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">nc</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Discriminator</span>
<span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ngpu</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Discriminator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">=</span> <span class="n">ngpu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">main</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">nc</span><span class="p">,</span> <span class="n">ndf</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">ndf</span><span class="p">,</span> <span class="n">ndf</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">ndf</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">ndf</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ndf</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">ndf</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">ndf</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Defining-the-Training-Function">
<a class="anchor" href="#Defining-the-Training-Function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Defining the Training Function<a class="anchor-link" href="#Defining-the-Training-Function"> </a>
</h2>
<p>The training function first trains the discriminator and then the generator as shown below. Note that by setting the real label value to <code>0.9</code> and the fake label value to <code>0.1</code>, I've applied label smoothing which has been shown to improve the results produced by the GAN.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">optimizerG</span><span class="p">,</span> <span class="n">optimizerD</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">iters</span><span class="p">):</span>
  <span class="n">gen</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
  <span class="n">disc</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
  <span class="n">img_list</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">fixed_noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">nz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

  <span class="c1"># Establish convention for real and fake labels during training (with label smoothing)</span>
  <span class="n">real_label</span> <span class="o">=</span> <span class="mf">0.9</span>
  <span class="n">fake_label</span> <span class="o">=</span> <span class="mf">0.1</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>

      <span class="c1">#*****</span>
      <span class="c1"># Update Discriminator</span>
      <span class="c1">#*****</span>
      <span class="c1">## Train with all-real batch</span>
      <span class="n">disc</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="c1"># Format batch</span>
      <span class="n">real_cpu</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
      <span class="n">b_size</span> <span class="o">=</span> <span class="n">real_cpu</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">b_size</span><span class="p">,),</span> <span class="n">real_label</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
      <span class="c1"># Forward pass real batch through D</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">real_cpu</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
      <span class="c1"># Calculate loss on all-real batch</span>
      <span class="n">errD_real</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
      <span class="c1"># Calculate gradients for D in backward pass</span>
      <span class="n">errD_real</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">D_x</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

      <span class="c1">## Train with all-fake batch</span>
      <span class="c1"># Generate batch of latent vectors</span>
      <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">b_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">nz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
      <span class="c1"># Generate fake image batch with G</span>
      <span class="n">fake</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
      <span class="n">label</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">fake_label</span><span class="p">)</span>
      <span class="c1"># Classify all fake batch with D</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">fake</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
      <span class="c1"># Calculate D's loss on the all-fake batch</span>
      <span class="n">errD_fake</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
      <span class="c1"># Calculate the gradients for this batch</span>
      <span class="n">errD_fake</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">D_G_z1</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
      <span class="c1"># Add the gradients from the all-real and all-fake batches</span>
      <span class="n">errD</span> <span class="o">=</span> <span class="n">errD_real</span> <span class="o">+</span> <span class="n">errD_fake</span>
      <span class="c1"># Update D</span>
      <span class="n">optimizerD</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

      <span class="c1">#*****</span>
      <span class="c1"># Update Generator</span>
      <span class="c1">#*****</span>
      <span class="n">gen</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">label</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">real_label</span><span class="p">)</span>  <span class="c1"># fake labels are real for generator cost</span>
      <span class="c1"># Since we just updated D, perform another forward pass of all-fake batch through D</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">fake</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
      <span class="c1"># Calculate G's loss based on this output</span>
      <span class="n">errG</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
      <span class="c1"># Calculate gradients for G</span>
      <span class="n">errG</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">D_G_z2</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
      <span class="c1"># Update G</span>
      <span class="n">optimizerG</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

      <span class="c1"># Output training stats</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="s1">'[</span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">][</span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">]</span><span class="se">\t</span><span class="s1">Loss_D: </span><span class="si">%.4f</span><span class="se">\t</span><span class="s1">Loss_G: </span><span class="si">%.4f</span><span class="se">\t</span><span class="s1">D(x): </span><span class="si">%.4f</span><span class="se">\t</span><span class="s1">D(G(z)): </span><span class="si">%.4f</span><span class="s1"> / </span><span class="si">%.4f</span><span class="s1">'</span>
                <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">),</span>
                    <span class="n">errD</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">errG</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">D_x</span><span class="p">,</span> <span class="n">D_G_z1</span><span class="p">,</span> <span class="n">D_G_z2</span><span class="p">))</span>
          <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span>
              <span class="s2">"Gen Loss"</span><span class="p">:</span> <span class="n">errG</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
              <span class="s2">"Disc Loss"</span><span class="p">:</span> <span class="n">errD</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>

      <span class="c1"># Check how the generator is doing by saving G's output on fixed_noise</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">iters</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">((</span><span class="n">epoch</span> <span class="o">==</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
          <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
              <span class="n">fake</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">fixed_noise</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
          <span class="n">img_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wandb</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">vutils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">fake</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>
          <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span>
              <span class="s2">"Generated Images"</span><span class="p">:</span> <span class="n">img_list</span><span class="p">})</span>
      <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Monitoring-the-Run">
<a class="anchor" href="#Monitoring-the-Run" aria-hidden="true"><span class="octicon octicon-link"></span></a>Monitoring the Run<a class="anchor-link" href="#Monitoring-the-Run"> </a>
</h2>
<p>Once we have all the pieces in place, all we need to do is train the model and watch it learn.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#hide-collapse</span>
<span class="n">wandb</span><span class="o">.</span><span class="n">watch_called</span> <span class="o">=</span> <span class="kc">False</span> 
<span class="c1"># WandB – Config is a variable that holds and saves hyperparameters and inputs</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">config</span>          <span class="c1"># Initialize config</span>
<span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> 
<span class="n">config</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">num_epochs</span>         
<span class="n">config</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>              
<span class="n">config</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">beta1</span>
<span class="n">config</span><span class="o">.</span><span class="n">nz</span> <span class="o">=</span> <span class="n">nz</span>          
<span class="n">config</span><span class="o">.</span><span class="n">no_cuda</span> <span class="o">=</span> <span class="kc">False</span>         
<span class="n">config</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">manualSeed</span> <span class="c1"># random seed (default: 42)</span>
<span class="n">config</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># how many batches to wait before logging training status</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">use_cuda</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">no_cuda</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'num_workers'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'pin_memory'</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="p">{}</span>
    
    <span class="c1"># Set random seeds and deterministic pytorch for reproducibility</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>       <span class="c1"># python random seed</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span> <span class="c1"># pytorch random seed</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span> <span class="c1"># numpy random seed</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Load the dataset</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
        <span class="p">[</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

    <span class="n">trainset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                            <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                                              <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">workers</span><span class="p">)</span>

    <span class="c1"># Create the generator</span>
    <span class="n">netG</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">ngpu</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Handle multi-gpu if desired</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'cuda'</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">netG</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ngpu</span><span class="p">)))</span>

    <span class="c1"># Apply the weights_init function to randomly initialize all weights</span>
    <span class="c1">#  to mean=0, stdev=0.2.</span>
    <span class="n">netG</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">weights_init</span><span class="p">)</span>

    <span class="c1"># Create the Discriminator</span>
    <span class="n">netD</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">(</span><span class="n">ngpu</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Handle multi-gpu if desired</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'cuda'</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">netD</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">netD</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ngpu</span><span class="p">)))</span>

    <span class="c1"># Apply the weights_init function to randomly initialize all weights</span>
    <span class="c1">#  to mean=0, stdev=0.2.</span>
    <span class="n">netD</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">weights_init</span><span class="p">)</span>

    <span class="c1"># Initialize BCELoss function</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>

    <span class="c1"># Setup Adam optimizers for both G and D</span>
    <span class="n">optimizerD</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">netD</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
    <span class="n">optimizerG</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">netG</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
    
    <span class="c1"># WandB – wandb.watch() automatically fetches all layer dimensions, gradients, model parameters and logs them automatically to your dashboard.</span>
    <span class="c1"># Using log="all" log histograms of parameter values in addition to gradients</span>
    <span class="n">wandb</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="s2">"all"</span><span class="p">)</span>
    <span class="n">wandb</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">netD</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="s2">"all"</span><span class="p">)</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">train</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">netG</span><span class="p">,</span> <span class="n">netD</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">trainloader</span><span class="p">,</span> <span class="n">optimizerG</span><span class="p">,</span> <span class="n">optimizerD</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">iters</span><span class="p">)</span>
        
    <span class="c1"># WandB – Save the model checkpoint. This automatically saves a file to the cloud and associates it with the current run.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">netG</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">"model.h5"</span><span class="p">)</span>
    <span class="n">wandb</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">'model.h5'</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">'__main__'</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Files already downloaded and verified
[1/30][0/391]	Loss_D: 1.5330	Loss_G: 2.0065	D(x): 0.4798	D(G(z)): 0.4893 / 0.1298
[1/30][50/391]	Loss_D: 0.8715	Loss_G: 4.9647	D(x): 0.7946	D(G(z)): 0.3070 / 0.0045
[1/30][100/391]	Loss_D: 0.7232	Loss_G: 3.8356	D(x): 0.8442	D(G(z)): 0.1959 / 0.0156
[1/30][150/391]	Loss_D: 0.8965	Loss_G: 2.2287	D(x): 0.7690	D(G(z)): 0.2954 / 0.0948
[1/30][200/391]	Loss_D: 0.9530	Loss_G: 1.8095	D(x): 0.6412	D(G(z)): 0.1987 / 0.1468
[1/30][250/391]	Loss_D: 0.9138	Loss_G: 2.8588	D(x): 0.6709	D(G(z)): 0.1570 / 0.0513
[1/30][300/391]	Loss_D: 0.8062	Loss_G: 2.6343	D(x): 0.7680	D(G(z)): 0.1848 / 0.0608
[1/30][350/391]	Loss_D: 1.1042	Loss_G: 1.6227	D(x): 0.5420	D(G(z)): 0.1842 / 0.1997
[2/30][0/391]	Loss_D: 1.0089	Loss_G: 1.7177	D(x): 0.5759	D(G(z)): 0.2028 / 0.1679
[2/30][50/391]	Loss_D: 0.9720	Loss_G: 2.1761	D(x): 0.7768	D(G(z)): 0.3881 / 0.0989
[2/30][100/391]	Loss_D: 0.9278	Loss_G: 2.0908	D(x): 0.7203	D(G(z)): 0.2952 / 0.1097
[2/30][150/391]	Loss_D: 0.8625	Loss_G: 2.2617	D(x): 0.7206	D(G(z)): 0.1978 / 0.0911
[2/30][200/391]	Loss_D: 1.1439	Loss_G: 2.3606	D(x): 0.6800	D(G(z)): 0.4345 / 0.0856
[2/30][250/391]	Loss_D: 0.8159	Loss_G: 2.0152	D(x): 0.7410	D(G(z)): 0.1987 / 0.1190
[2/30][300/391]	Loss_D: 0.8544	Loss_G: 2.1243	D(x): 0.6887	D(G(z)): 0.1167 / 0.1094
[2/30][350/391]	Loss_D: 0.8503	Loss_G: 2.0028	D(x): 0.7512	D(G(z)): 0.2479 / 0.1256
[3/30][0/391]	Loss_D: 1.0292	Loss_G: 2.3918	D(x): 0.8430	D(G(z)): 0.4425 / 0.0835
[3/30][50/391]	Loss_D: 0.8971	Loss_G: 1.8851	D(x): 0.6703	D(G(z)): 0.1654 / 0.1446
[3/30][100/391]	Loss_D: 1.0793	Loss_G: 1.3440	D(x): 0.5632	D(G(z)): 0.1374 / 0.2698
[3/30][150/391]	Loss_D: 0.8603	Loss_G: 2.2827	D(x): 0.7910	D(G(z)): 0.2611 / 0.0984
[3/30][200/391]	Loss_D: 0.9094	Loss_G: 2.0252	D(x): 0.7199	D(G(z)): 0.2683 / 0.1289
[3/30][250/391]	Loss_D: 0.8966	Loss_G: 2.1354	D(x): 0.7764	D(G(z)): 0.2912 / 0.1119
[3/30][300/391]	Loss_D: 0.8312	Loss_G: 1.7893	D(x): 0.7383	D(G(z)): 0.2004 / 0.1578
[3/30][350/391]	Loss_D: 1.0215	Loss_G: 1.3776	D(x): 0.6032	D(G(z)): 0.2551 / 0.2479
[4/30][0/391]	Loss_D: 0.9510	Loss_G: 2.0825	D(x): 0.7698	D(G(z)): 0.3555 / 0.1153
[4/30][50/391]	Loss_D: 0.8927	Loss_G: 1.9853	D(x): 0.7803	D(G(z)): 0.3192 / 0.1255
[4/30][100/391]	Loss_D: 0.8228	Loss_G: 1.5415	D(x): 0.7643	D(G(z)): 0.2403 / 0.2008
[4/30][150/391]	Loss_D: 0.8701	Loss_G: 1.8234	D(x): 0.7514	D(G(z)): 0.2680 / 0.1555
[4/30][200/391]	Loss_D: 0.8611	Loss_G: 1.5613	D(x): 0.6997	D(G(z)): 0.1834 / 0.2065
[4/30][250/391]	Loss_D: 0.7854	Loss_G: 2.0144	D(x): 0.8383	D(G(z)): 0.2203 / 0.1215
[4/30][300/391]	Loss_D: 0.7546	Loss_G: 2.0268	D(x): 0.7897	D(G(z)): 0.1478 / 0.1218
[4/30][350/391]	Loss_D: 0.9487	Loss_G: 1.3846	D(x): 0.6133	D(G(z)): 0.1861 / 0.2547
[5/30][0/391]	Loss_D: 1.0132	Loss_G: 0.7042	D(x): 0.5492	D(G(z)): 0.1104 / 0.5286
[5/30][50/391]	Loss_D: 0.9648	Loss_G: 1.9506	D(x): 0.7825	D(G(z)): 0.3778 / 0.1327
[5/30][100/391]	Loss_D: 0.8504	Loss_G: 1.5864	D(x): 0.7236	D(G(z)): 0.2211 / 0.1928
[5/30][150/391]	Loss_D: 0.9893	Loss_G: 1.4641	D(x): 0.7339	D(G(z)): 0.3483 / 0.2283
[5/30][200/391]	Loss_D: 0.9685	Loss_G: 1.4706	D(x): 0.6254	D(G(z)): 0.2019 / 0.2398
[5/30][250/391]	Loss_D: 0.9474	Loss_G: 1.9077	D(x): 0.7109	D(G(z)): 0.3222 / 0.1389
[5/30][300/391]	Loss_D: 1.0227	Loss_G: 1.4165	D(x): 0.6255	D(G(z)): 0.2816 / 0.2360
[5/30][350/391]	Loss_D: 1.1103	Loss_G: 1.0159	D(x): 0.5182	D(G(z)): 0.1977 / 0.3701
[6/30][0/391]	Loss_D: 0.8827	Loss_G: 1.6163	D(x): 0.6925	D(G(z)): 0.2418 / 0.1865
[6/30][50/391]	Loss_D: 1.1054	Loss_G: 2.2958	D(x): 0.8291	D(G(z)): 0.4917 / 0.0939
[6/30][100/391]	Loss_D: 0.9841	Loss_G: 1.4822	D(x): 0.6658	D(G(z)): 0.3004 / 0.2176
[6/30][150/391]	Loss_D: 0.9598	Loss_G: 1.5836	D(x): 0.7312	D(G(z)): 0.3420 / 0.1988
[6/30][200/391]	Loss_D: 1.0637	Loss_G: 1.3479	D(x): 0.6263	D(G(z)): 0.3189 / 0.2635
[6/30][250/391]	Loss_D: 1.0678	Loss_G: 1.8553	D(x): 0.7680	D(G(z)): 0.4433 / 0.1513
[6/30][300/391]	Loss_D: 1.0556	Loss_G: 1.8421	D(x): 0.7322	D(G(z)): 0.4143 / 0.1496
[6/30][350/391]	Loss_D: 1.0985	Loss_G: 1.9509	D(x): 0.7936	D(G(z)): 0.4807 / 0.1308
[7/30][0/391]	Loss_D: 0.9637	Loss_G: 1.3168	D(x): 0.6677	D(G(z)): 0.2981 / 0.2594
[7/30][50/391]	Loss_D: 1.0530	Loss_G: 1.1391	D(x): 0.5883	D(G(z)): 0.2867 / 0.3141
[7/30][100/391]	Loss_D: 0.9574	Loss_G: 1.4361	D(x): 0.6782	D(G(z)): 0.3015 / 0.2262
[7/30][150/391]	Loss_D: 1.0266	Loss_G: 1.4384	D(x): 0.6787	D(G(z)): 0.3633 / 0.2260
[7/30][200/391]	Loss_D: 1.2161	Loss_G: 2.5106	D(x): 0.7031	D(G(z)): 0.5100 / 0.0720
[7/30][250/391]	Loss_D: 0.9846	Loss_G: 1.5405	D(x): 0.7217	D(G(z)): 0.3592 / 0.2093
[7/30][300/391]	Loss_D: 1.0500	Loss_G: 1.0985	D(x): 0.5870	D(G(z)): 0.2566 / 0.3348
[7/30][350/391]	Loss_D: 0.9962	Loss_G: 1.3818	D(x): 0.6454	D(G(z)): 0.3064 / 0.2391
[8/30][0/391]	Loss_D: 1.5409	Loss_G: 0.8797	D(x): 0.3085	D(G(z)): 0.1152 / 0.4338
[8/30][50/391]	Loss_D: 1.1932	Loss_G: 1.0164	D(x): 0.4724	D(G(z)): 0.1956 / 0.3694
[8/30][100/391]	Loss_D: 1.1094	Loss_G: 1.9686	D(x): 0.8014	D(G(z)): 0.4915 / 0.1341
[8/30][150/391]	Loss_D: 1.0402	Loss_G: 1.0891	D(x): 0.5796	D(G(z)): 0.2517 / 0.3365
[8/30][200/391]	Loss_D: 1.0166	Loss_G: 1.1981	D(x): 0.6393	D(G(z)): 0.3064 / 0.2972
[8/30][250/391]	Loss_D: 1.3140	Loss_G: 2.1779	D(x): 0.7989	D(G(z)): 0.5972 / 0.1066
[8/30][300/391]	Loss_D: 1.0808	Loss_G: 1.3231	D(x): 0.6657	D(G(z)): 0.3929 / 0.2597
[8/30][350/391]	Loss_D: 0.9214	Loss_G: 1.6856	D(x): 0.7807	D(G(z)): 0.3585 / 0.1708
[9/30][0/391]	Loss_D: 1.0025	Loss_G: 1.4469	D(x): 0.6824	D(G(z)): 0.3375 / 0.2321
[9/30][50/391]	Loss_D: 1.3056	Loss_G: 1.2976	D(x): 0.6130	D(G(z)): 0.5060 / 0.2650
[9/30][100/391]	Loss_D: 0.9936	Loss_G: 1.3405	D(x): 0.5984	D(G(z)): 0.2396 / 0.2562
[9/30][150/391]	Loss_D: 1.2408	Loss_G: 1.0987	D(x): 0.4876	D(G(z)): 0.3149 / 0.3332
[9/30][200/391]	Loss_D: 0.9973	Loss_G: 1.4097	D(x): 0.6510	D(G(z)): 0.3216 / 0.2331
[9/30][250/391]	Loss_D: 1.0263	Loss_G: 1.2741	D(x): 0.6090	D(G(z)): 0.2867 / 0.2714
[9/30][300/391]	Loss_D: 1.1614	Loss_G: 2.1137	D(x): 0.8225	D(G(z)): 0.5258 / 0.1121
[9/30][350/391]	Loss_D: 1.0194	Loss_G: 1.0657	D(x): 0.6055	D(G(z)): 0.2866 / 0.3395
[10/30][0/391]	Loss_D: 1.0612	Loss_G: 1.2795	D(x): 0.6027	D(G(z)): 0.3106 / 0.2702
[10/30][50/391]	Loss_D: 1.1699	Loss_G: 1.0217	D(x): 0.5101	D(G(z)): 0.2744 / 0.3589
[10/30][100/391]	Loss_D: 1.0647	Loss_G: 1.7456	D(x): 0.7280	D(G(z)): 0.4372 / 0.1606
[10/30][150/391]	Loss_D: 1.2592	Loss_G: 0.8300	D(x): 0.4822	D(G(z)): 0.3269 / 0.4461
[10/30][200/391]	Loss_D: 0.9582	Loss_G: 1.8927	D(x): 0.7721	D(G(z)): 0.3860 / 0.1358
[10/30][250/391]	Loss_D: 1.0677	Loss_G: 1.4276	D(x): 0.6790	D(G(z)): 0.4053 / 0.2250
[10/30][300/391]	Loss_D: 1.1234	Loss_G: 1.2369	D(x): 0.5594	D(G(z)): 0.3270 / 0.2799
[10/30][350/391]	Loss_D: 1.0698	Loss_G: 1.6208	D(x): 0.7850	D(G(z)): 0.4724 / 0.1830
[11/30][0/391]	Loss_D: 0.9760	Loss_G: 1.2819	D(x): 0.6136	D(G(z)): 0.2478 / 0.2686
[11/30][50/391]	Loss_D: 0.9170	Loss_G: 1.3954	D(x): 0.6942	D(G(z)): 0.2872 / 0.2354
[11/30][100/391]	Loss_D: 0.9664	Loss_G: 1.2902	D(x): 0.6745	D(G(z)): 0.3175 / 0.2676
[11/30][150/391]	Loss_D: 1.0373	Loss_G: 1.1947	D(x): 0.6357	D(G(z)): 0.3377 / 0.2964
[11/30][200/391]	Loss_D: 1.0634	Loss_G: 1.6096	D(x): 0.6739	D(G(z)): 0.3937 / 0.1897
[11/30][250/391]	Loss_D: 0.9452	Loss_G: 1.7660	D(x): 0.7718	D(G(z)): 0.3655 / 0.1644
[11/30][300/391]	Loss_D: 1.0072	Loss_G: 1.3114	D(x): 0.5903	D(G(z)): 0.2331 / 0.2578
[11/30][350/391]	Loss_D: 1.8891	Loss_G: 0.3712	D(x): 0.2055	D(G(z)): 0.0591 / 0.9124
[12/30][0/391]	Loss_D: 0.9950	Loss_G: 1.4577	D(x): 0.6965	D(G(z)): 0.3567 / 0.2259
[12/30][50/391]	Loss_D: 1.0831	Loss_G: 1.0768	D(x): 0.5093	D(G(z)): 0.1850 / 0.3359
[12/30][100/391]	Loss_D: 0.9436	Loss_G: 1.3149	D(x): 0.5983	D(G(z)): 0.1752 / 0.2583
[12/30][150/391]	Loss_D: 0.9256	Loss_G: 1.5862	D(x): 0.7769	D(G(z)): 0.3604 / 0.1917
[12/30][200/391]	Loss_D: 1.5656	Loss_G: 1.7253	D(x): 0.8284	D(G(z)): 0.6978 / 0.1704
[12/30][250/391]	Loss_D: 1.1823	Loss_G: 1.3516	D(x): 0.6949	D(G(z)): 0.4968 / 0.2446
[12/30][300/391]	Loss_D: 1.0467	Loss_G: 1.4240	D(x): 0.7208	D(G(z)): 0.4215 / 0.2271
[12/30][350/391]	Loss_D: 1.0150	Loss_G: 1.5902	D(x): 0.7788	D(G(z)): 0.4328 / 0.1881
[13/30][0/391]	Loss_D: 1.0308	Loss_G: 1.4838	D(x): 0.7335	D(G(z)): 0.4172 / 0.2118
[13/30][50/391]	Loss_D: 1.0585	Loss_G: 1.1484	D(x): 0.5756	D(G(z)): 0.2751 / 0.3161
[13/30][100/391]	Loss_D: 1.0553	Loss_G: 1.5469	D(x): 0.6518	D(G(z)): 0.3704 / 0.2003
[13/30][150/391]	Loss_D: 1.0285	Loss_G: 1.3155	D(x): 0.6715	D(G(z)): 0.3604 / 0.2569
[13/30][200/391]	Loss_D: 1.0419	Loss_G: 1.1947	D(x): 0.6208	D(G(z)): 0.3158 / 0.3006
[13/30][250/391]	Loss_D: 1.3148	Loss_G: 0.3872	D(x): 0.4021	D(G(z)): 0.2104 / 0.7859
[13/30][300/391]	Loss_D: 0.9487	Loss_G: 1.4255	D(x): 0.6913	D(G(z)): 0.3121 / 0.2279
[13/30][350/391]	Loss_D: 1.1076	Loss_G: 1.0830	D(x): 0.5353	D(G(z)): 0.2674 / 0.3348
[14/30][0/391]	Loss_D: 1.0612	Loss_G: 1.0598	D(x): 0.5697	D(G(z)): 0.2760 / 0.3424
[14/30][50/391]	Loss_D: 1.0218	Loss_G: 1.9751	D(x): 0.7966	D(G(z)): 0.4417 / 0.1271
[14/30][100/391]	Loss_D: 1.3000	Loss_G: 0.5739	D(x): 0.3979	D(G(z)): 0.1513 / 0.6069
[14/30][150/391]	Loss_D: 0.9931	Loss_G: 1.8432	D(x): 0.7729	D(G(z)): 0.4132 / 0.1446
[14/30][200/391]	Loss_D: 1.1232	Loss_G: 1.1526	D(x): 0.5320	D(G(z)): 0.2618 / 0.3120
[14/30][250/391]	Loss_D: 1.1957	Loss_G: 1.4379	D(x): 0.6904	D(G(z)): 0.4878 / 0.2271
[14/30][300/391]	Loss_D: 1.0007	Loss_G: 1.2963	D(x): 0.6535	D(G(z)): 0.3146 / 0.2652
[14/30][350/391]	Loss_D: 1.0849	Loss_G: 1.0480	D(x): 0.5315	D(G(z)): 0.2361 / 0.3496
[15/30][0/391]	Loss_D: 1.0305	Loss_G: 1.3248	D(x): 0.6690	D(G(z)): 0.3635 / 0.2535
[15/30][50/391]	Loss_D: 1.3088	Loss_G: 0.5679	D(x): 0.3888	D(G(z)): 0.1352 / 0.6100
[15/30][100/391]	Loss_D: 1.0869	Loss_G: 1.4303	D(x): 0.6516	D(G(z)): 0.3942 / 0.2296
[15/30][150/391]	Loss_D: 1.1082	Loss_G: 0.9655	D(x): 0.5181	D(G(z)): 0.2441 / 0.3799
[15/30][200/391]	Loss_D: 1.1288	Loss_G: 1.5768	D(x): 0.7549	D(G(z)): 0.4903 / 0.1930
[15/30][250/391]	Loss_D: 1.6270	Loss_G: 0.8472	D(x): 0.4068	D(G(z)): 0.4135 / 0.4905
[15/30][300/391]	Loss_D: 1.2084	Loss_G: 1.0170	D(x): 0.5059	D(G(z)): 0.3147 / 0.3601
[15/30][350/391]	Loss_D: 1.0819	Loss_G: 1.1340	D(x): 0.5780	D(G(z)): 0.3061 / 0.3156
[16/30][0/391]	Loss_D: 1.1331	Loss_G: 1.4749	D(x): 0.6504	D(G(z)): 0.4252 / 0.2184
[16/30][50/391]	Loss_D: 1.0175	Loss_G: 1.0656	D(x): 0.5682	D(G(z)): 0.2266 / 0.3423
[16/30][100/391]	Loss_D: 1.0190	Loss_G: 1.6862	D(x): 0.7375	D(G(z)): 0.4017 / 0.1736
[16/30][150/391]	Loss_D: 1.0497	Loss_G: 1.1752	D(x): 0.5856	D(G(z)): 0.2815 / 0.3090
[16/30][200/391]	Loss_D: 1.6160	Loss_G: 0.3481	D(x): 0.2845	D(G(z)): 0.1125 / 0.8798
[16/30][250/391]	Loss_D: 1.0176	Loss_G: 1.2730	D(x): 0.6313	D(G(z)): 0.3164 / 0.2637
[16/30][300/391]	Loss_D: 1.0558	Loss_G: 1.0953	D(x): 0.5947	D(G(z)): 0.3055 / 0.3273
[16/30][350/391]	Loss_D: 1.1903	Loss_G: 1.2424	D(x): 0.6171	D(G(z)): 0.4396 / 0.2823
[17/30][0/391]	Loss_D: 1.0670	Loss_G: 1.3721	D(x): 0.6998	D(G(z)): 0.4217 / 0.2446
[17/30][50/391]	Loss_D: 0.9566	Loss_G: 1.2191	D(x): 0.6072	D(G(z)): 0.2059 / 0.2872
[17/30][100/391]	Loss_D: 1.1072	Loss_G: 1.0355	D(x): 0.5674	D(G(z)): 0.3136 / 0.3509
[17/30][150/391]	Loss_D: 1.5534	Loss_G: 1.6169	D(x): 0.7082	D(G(z)): 0.6704 / 0.1896
[17/30][200/391]	Loss_D: 1.0195	Loss_G: 1.1475	D(x): 0.6366	D(G(z)): 0.3214 / 0.3176
[17/30][250/391]	Loss_D: 1.0278	Loss_G: 1.2424	D(x): 0.6161	D(G(z)): 0.3034 / 0.2853
[17/30][300/391]	Loss_D: 0.9902	Loss_G: 1.4736	D(x): 0.7227	D(G(z)): 0.3792 / 0.2160
[17/30][350/391]	Loss_D: 1.1040	Loss_G: 0.9200	D(x): 0.5122	D(G(z)): 0.2319 / 0.4013
[18/30][0/391]	Loss_D: 1.0960	Loss_G: 1.0392	D(x): 0.5725	D(G(z)): 0.3085 / 0.3482
[18/30][50/391]	Loss_D: 0.9788	Loss_G: 1.5755	D(x): 0.7268	D(G(z)): 0.3811 / 0.1882
[18/30][100/391]	Loss_D: 0.8997	Loss_G: 1.5019	D(x): 0.7111	D(G(z)): 0.2917 / 0.2094
[18/30][150/391]	Loss_D: 1.2186	Loss_G: 1.7073	D(x): 0.7189	D(G(z)): 0.5183 / 0.1728
[18/30][200/391]	Loss_D: 1.3071	Loss_G: 0.8504	D(x): 0.4213	D(G(z)): 0.2571 / 0.4327
[18/30][250/391]	Loss_D: 1.1458	Loss_G: 0.8859	D(x): 0.5040	D(G(z)): 0.2565 / 0.4144
[18/30][300/391]	Loss_D: 1.0417	Loss_G: 0.9717	D(x): 0.5995	D(G(z)): 0.3028 / 0.3742
[18/30][350/391]	Loss_D: 1.0133	Loss_G: 1.3466	D(x): 0.7132	D(G(z)): 0.3863 / 0.2500
[19/30][0/391]	Loss_D: 1.3726	Loss_G: 2.0758	D(x): 0.7981	D(G(z)): 0.6310 / 0.1142
[19/30][50/391]	Loss_D: 0.9745	Loss_G: 1.5558	D(x): 0.7433	D(G(z)): 0.3790 / 0.1957
[19/30][100/391]	Loss_D: 1.0528	Loss_G: 0.9203	D(x): 0.5669	D(G(z)): 0.2604 / 0.4082
[19/30][150/391]	Loss_D: 1.5363	Loss_G: 0.8982	D(x): 0.4001	D(G(z)): 0.4051 / 0.4157
[19/30][200/391]	Loss_D: 1.1483	Loss_G: 0.7528	D(x): 0.4958	D(G(z)): 0.2292 / 0.4856
[19/30][250/391]	Loss_D: 1.0381	Loss_G: 1.3302	D(x): 0.6641	D(G(z)): 0.3737 / 0.2531
[19/30][300/391]	Loss_D: 1.1083	Loss_G: 0.9069	D(x): 0.5452	D(G(z)): 0.2756 / 0.4079
[19/30][350/391]	Loss_D: 1.1544	Loss_G: 1.2182	D(x): 0.6001	D(G(z)): 0.3961 / 0.2831
[20/30][0/391]	Loss_D: 1.0190	Loss_G: 1.2576	D(x): 0.6303	D(G(z)): 0.3136 / 0.2755
[20/30][50/391]	Loss_D: 1.0354	Loss_G: 1.5062	D(x): 0.6917	D(G(z)): 0.3959 / 0.2054
[20/30][100/391]	Loss_D: 1.2535	Loss_G: 0.7139	D(x): 0.4359	D(G(z)): 0.2179 / 0.5107
[20/30][150/391]	Loss_D: 1.0953	Loss_G: 1.2183	D(x): 0.6362	D(G(z)): 0.3834 / 0.2883
[20/30][200/391]	Loss_D: 1.0546	Loss_G: 1.3230	D(x): 0.7128	D(G(z)): 0.4188 / 0.2540
[20/30][250/391]	Loss_D: 1.0512	Loss_G: 1.1199	D(x): 0.5592	D(G(z)): 0.2516 / 0.3177
[20/30][300/391]	Loss_D: 1.0122	Loss_G: 1.3715	D(x): 0.7465	D(G(z)): 0.4099 / 0.2427
[20/30][350/391]	Loss_D: 1.0544	Loss_G: 1.3656	D(x): 0.6835	D(G(z)): 0.3958 / 0.2388
[21/30][0/391]	Loss_D: 1.0685	Loss_G: 1.8458	D(x): 0.7763	D(G(z)): 0.4662 / 0.1463
[21/30][50/391]	Loss_D: 1.0744	Loss_G: 1.2440	D(x): 0.6764	D(G(z)): 0.4115 / 0.2804
[21/30][100/391]	Loss_D: 1.0559	Loss_G: 1.3944	D(x): 0.6855	D(G(z)): 0.4008 / 0.2356
[21/30][150/391]	Loss_D: 1.1035	Loss_G: 1.1176	D(x): 0.5713	D(G(z)): 0.3236 / 0.3213
[21/30][200/391]	Loss_D: 1.0946	Loss_G: 1.0384	D(x): 0.5235	D(G(z)): 0.2393 / 0.3490
[21/30][250/391]	Loss_D: 1.5409	Loss_G: 2.3917	D(x): 0.8745	D(G(z)): 0.6994 / 0.0843
[21/30][300/391]	Loss_D: 1.1315	Loss_G: 0.9393	D(x): 0.5008	D(G(z)): 0.2352 / 0.3872
[21/30][350/391]	Loss_D: 1.0280	Loss_G: 1.2758	D(x): 0.6088	D(G(z)): 0.2952 / 0.2737
[22/30][0/391]	Loss_D: 1.1280	Loss_G: 1.4459	D(x): 0.6356	D(G(z)): 0.3952 / 0.2246
[22/30][50/391]	Loss_D: 1.1620	Loss_G: 0.9946	D(x): 0.5696	D(G(z)): 0.3604 / 0.3703
[22/30][100/391]	Loss_D: 0.9770	Loss_G: 1.4543	D(x): 0.6995	D(G(z)): 0.3471 / 0.2229
[22/30][150/391]	Loss_D: 1.0864	Loss_G: 1.3690	D(x): 0.6791	D(G(z)): 0.4188 / 0.2381
[22/30][200/391]	Loss_D: 1.2507	Loss_G: 0.7848	D(x): 0.4238	D(G(z)): 0.1824 / 0.4673
[22/30][250/391]	Loss_D: 1.4277	Loss_G: 1.1070	D(x): 0.6578	D(G(z)): 0.5827 / 0.3285
[22/30][300/391]	Loss_D: 1.1703	Loss_G: 2.1975	D(x): 0.8293	D(G(z)): 0.5382 / 0.1004
[22/30][350/391]	Loss_D: 1.4329	Loss_G: 3.3520	D(x): 0.7813	D(G(z)): 0.6275 / 0.0304
[23/30][0/391]	Loss_D: 1.1567	Loss_G: 1.1079	D(x): 0.5522	D(G(z)): 0.3503 / 0.3193
[23/30][50/391]	Loss_D: 1.1654	Loss_G: 1.4560	D(x): 0.7406	D(G(z)): 0.5053 / 0.2225
[23/30][100/391]	Loss_D: 1.0584	Loss_G: 1.2907	D(x): 0.6146	D(G(z)): 0.3307 / 0.2639
[23/30][150/391]	Loss_D: 1.1503	Loss_G: 0.9929	D(x): 0.4971	D(G(z)): 0.2458 / 0.3717
[23/30][200/391]	Loss_D: 1.0898	Loss_G: 1.6250	D(x): 0.7449	D(G(z)): 0.4576 / 0.1812
[23/30][250/391]	Loss_D: 1.2575	Loss_G: 0.6943	D(x): 0.4095	D(G(z)): 0.1767 / 0.5149
[23/30][300/391]	Loss_D: 1.0266	Loss_G: 1.2971	D(x): 0.6639	D(G(z)): 0.3590 / 0.2624
[23/30][350/391]	Loss_D: 1.0735	Loss_G: 1.2237	D(x): 0.5776	D(G(z)): 0.3023 / 0.2847
[24/30][0/391]	Loss_D: 1.1004	Loss_G: 1.1137	D(x): 0.5817	D(G(z)): 0.3335 / 0.3219
[24/30][50/391]	Loss_D: 2.0557	Loss_G: 0.4472	D(x): 0.1792	D(G(z)): 0.1065 / 0.7225
[24/30][100/391]	Loss_D: 1.0702	Loss_G: 1.1436	D(x): 0.6057	D(G(z)): 0.3222 / 0.3111
[24/30][150/391]	Loss_D: 0.9627	Loss_G: 1.4126	D(x): 0.6567	D(G(z)): 0.2861 / 0.2377
[24/30][200/391]	Loss_D: 0.9807	Loss_G: 1.4336	D(x): 0.6841	D(G(z)): 0.3353 / 0.2267
[24/30][250/391]	Loss_D: 1.0433	Loss_G: 1.0720	D(x): 0.6218	D(G(z)): 0.3217 / 0.3369
[24/30][300/391]	Loss_D: 1.0439	Loss_G: 1.4228	D(x): 0.6963	D(G(z)): 0.4011 / 0.2277
[24/30][350/391]	Loss_D: 1.0372	Loss_G: 0.9824	D(x): 0.5723	D(G(z)): 0.2579 / 0.3742
[25/30][0/391]	Loss_D: 1.1856	Loss_G: 1.1063	D(x): 0.6386	D(G(z)): 0.4578 / 0.3232
[25/30][50/391]	Loss_D: 1.0876	Loss_G: 1.1943	D(x): 0.6299	D(G(z)): 0.3757 / 0.2938
[25/30][100/391]	Loss_D: 1.1746	Loss_G: 1.2097	D(x): 0.4657	D(G(z)): 0.1787 / 0.3032
[25/30][150/391]	Loss_D: 1.0051	Loss_G: 1.1847	D(x): 0.6147	D(G(z)): 0.2745 / 0.3018
[25/30][200/391]	Loss_D: 1.2852	Loss_G: 0.9885	D(x): 0.4231	D(G(z)): 0.2150 / 0.3709
[25/30][250/391]	Loss_D: 1.0958	Loss_G: 1.7921	D(x): 0.6723	D(G(z)): 0.4034 / 0.1603
[25/30][300/391]	Loss_D: 1.0115	Loss_G: 1.4030	D(x): 0.7162	D(G(z)): 0.3908 / 0.2361
[25/30][350/391]	Loss_D: 0.9268	Loss_G: 1.5395	D(x): 0.7064	D(G(z)): 0.3069 / 0.2030
[26/30][0/391]	Loss_D: 1.0486	Loss_G: 0.9544	D(x): 0.5362	D(G(z)): 0.1967 / 0.3835
[26/30][50/391]	Loss_D: 1.0524	Loss_G: 1.1044	D(x): 0.6183	D(G(z)): 0.3383 / 0.3202
[26/30][100/391]	Loss_D: 1.4844	Loss_G: 2.2050	D(x): 0.8661	D(G(z)): 0.6792 / 0.1010
[26/30][150/391]	Loss_D: 1.0594	Loss_G: 1.3569	D(x): 0.6854	D(G(z)): 0.4001 / 0.2443
[26/30][200/391]	Loss_D: 0.9221	Loss_G: 1.6898	D(x): 0.7913	D(G(z)): 0.3668 / 0.1734
[26/30][250/391]	Loss_D: 1.0627	Loss_G: 1.1584	D(x): 0.5780	D(G(z)): 0.2944 / 0.3073
[26/30][300/391]	Loss_D: 1.0118	Loss_G: 0.7999	D(x): 0.5995	D(G(z)): 0.2603 / 0.4587
[26/30][350/391]	Loss_D: 1.0222	Loss_G: 1.3166	D(x): 0.7171	D(G(z)): 0.4007 / 0.2560
[27/30][0/391]	Loss_D: 1.0684	Loss_G: 1.5186	D(x): 0.7177	D(G(z)): 0.4342 / 0.2081
[27/30][50/391]	Loss_D: 0.9575	Loss_G: 1.1518	D(x): 0.6267	D(G(z)): 0.2523 / 0.3079
[27/30][100/391]	Loss_D: 1.2484	Loss_G: 1.5434	D(x): 0.7478	D(G(z)): 0.5502 / 0.2036
[27/30][150/391]	Loss_D: 1.0930	Loss_G: 1.9262	D(x): 0.8013	D(G(z)): 0.4914 / 0.1325
[27/30][200/391]	Loss_D: 0.9854	Loss_G: 1.3368	D(x): 0.6414	D(G(z)): 0.2801 / 0.2528
[27/30][250/391]	Loss_D: 1.1274	Loss_G: 0.8990	D(x): 0.5336	D(G(z)): 0.2889 / 0.4060
[27/30][300/391]	Loss_D: 1.1298	Loss_G: 1.3775	D(x): 0.7048	D(G(z)): 0.4627 / 0.2430
[27/30][350/391]	Loss_D: 1.1789	Loss_G: 1.0439	D(x): 0.5592	D(G(z)): 0.3604 / 0.3515
[28/30][0/391]	Loss_D: 1.2327	Loss_G: 0.4293	D(x): 0.4363	D(G(z)): 0.1641 / 0.7413
[28/30][50/391]	Loss_D: 1.0969	Loss_G: 1.7167	D(x): 0.7227	D(G(z)): 0.4571 / 0.1647
[28/30][100/391]	Loss_D: 1.2208	Loss_G: 1.8356	D(x): 0.7656	D(G(z)): 0.5422 / 0.1485
[28/30][150/391]	Loss_D: 0.9899	Loss_G: 1.2459	D(x): 0.6296	D(G(z)): 0.2899 / 0.2780
[28/30][200/391]	Loss_D: 1.1061	Loss_G: 1.2617	D(x): 0.7131	D(G(z)): 0.4436 / 0.2766
[28/30][250/391]	Loss_D: 1.0123	Loss_G: 1.5527	D(x): 0.7465	D(G(z)): 0.4134 / 0.1986
[28/30][300/391]	Loss_D: 1.0897	Loss_G: 1.3486	D(x): 0.7490	D(G(z)): 0.4658 / 0.2474
[28/30][350/391]	Loss_D: 1.2090	Loss_G: 1.5242	D(x): 0.7159	D(G(z)): 0.5127 / 0.2085
[29/30][0/391]	Loss_D: 1.0897	Loss_G: 1.8636	D(x): 0.8098	D(G(z)): 0.4832 / 0.1450
[29/30][50/391]	Loss_D: 0.9900	Loss_G: 1.1591	D(x): 0.6199	D(G(z)): 0.2589 / 0.3121
[29/30][100/391]	Loss_D: 0.9487	Loss_G: 1.1538	D(x): 0.6007	D(G(z)): 0.1828 / 0.3089
[29/30][150/391]	Loss_D: 1.1695	Loss_G: 0.9479	D(x): 0.5475	D(G(z)): 0.3373 / 0.3911
[29/30][200/391]	Loss_D: 0.9855	Loss_G: 1.2028	D(x): 0.6059	D(G(z)): 0.2403 / 0.2930
[29/30][250/391]	Loss_D: 0.9087	Loss_G: 1.2191	D(x): 0.6626	D(G(z)): 0.2366 / 0.2860
[29/30][300/391]	Loss_D: 1.1042	Loss_G: 1.9367	D(x): 0.8288	D(G(z)): 0.5125 / 0.1322
[29/30][350/391]	Loss_D: 1.1310	Loss_G: 0.9877	D(x): 0.5493	D(G(z)): 0.3014 / 0.3761
[30/30][0/391]	Loss_D: 0.9217	Loss_G: 1.0368	D(x): 0.6388	D(G(z)): 0.2206 / 0.3460
[30/30][50/391]	Loss_D: 1.3077	Loss_G: 2.0492	D(x): 0.7052	D(G(z)): 0.5517 / 0.1267
[30/30][100/391]	Loss_D: 1.0263	Loss_G: 1.0027	D(x): 0.5979	D(G(z)): 0.2794 / 0.3663
[30/30][150/391]	Loss_D: 1.3014	Loss_G: 0.6843	D(x): 0.3940	D(G(z)): 0.1646 / 0.5306
[30/30][200/391]	Loss_D: 1.3951	Loss_G: 0.9962	D(x): 0.6439	D(G(z)): 0.5578 / 0.3756
[30/30][250/391]	Loss_D: 1.0112	Loss_G: 1.5962	D(x): 0.7862	D(G(z)): 0.4260 / 0.1940
[30/30][300/391]	Loss_D: 1.1134	Loss_G: 1.4656	D(x): 0.6945	D(G(z)): 0.4396 / 0.2217
[30/30][350/391]	Loss_D: 1.0365	Loss_G: 1.6098	D(x): 0.7294	D(G(z)): 0.4134 / 0.1910
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loss-Curve-and-Results">
<a class="anchor" href="#Loss-Curve-and-Results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loss Curve and Results<a class="anchor-link" href="#Loss-Curve-and-Results"> </a>
</h2>
<p>For a few lines of code, the GAN produces pretty decent images in 30 epochs as you can see below.</p>
<h3 id="Loss">
<a class="anchor" href="#Loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loss<a class="anchor-link" href="#Loss"> </a>
</h3>
<p><img src="/vision-and-words/images/copied_from_nb/images/LossCurve.png" alt="" title="Loss Curves"></p>
<h3 id="Images">
<a class="anchor" href="#Images" aria-hidden="true"><span class="octicon octicon-link"></span></a>Images<a class="anchor-link" href="#Images"> </a>
</h3>
<p><img src="/vision-and-words/images/copied_from_nb/images/dcgan.png" alt="" title="Images after 30 epochs"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">
<a class="anchor" href="#References" aria-hidden="true"><span class="octicon octicon-link"></span></a>References<a class="anchor-link" href="#References"> </a>
</h2>
<ol>
<li>DCGAN Pytorch Tutorial: <a href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html">https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html</a>
</li>
</ol>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ssundar6087/vision-and-words"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/vision-and-words/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/vision-and-words/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/vision-and-words/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/vision-and-words/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog of the confused, by the confused for the confused in an attempt to reduce confusion.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/ssundar6087" title="ssundar6087"><svg class="svg-icon grey"><use xlink:href="/vision-and-words/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/%40dsaience" title="@dsaience"><svg class="svg-icon grey"><use xlink:href="/vision-and-words/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
