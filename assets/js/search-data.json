{
  
    
        "post0": {
            "title": "Implementing a Simple DCGAN in Pytorch",
            "content": "Overview . Below is a short overview of the key features of a DCGAN . Basic GAN Loss Function: . $ underset{G}{ text{min}} underset{D}{ text{max}}V(D,G) = mathbb{E}_{x sim p_{data}(x)} big[logD(x) big] + mathbb{E}_{z sim p_{z}(z)} big[log(1-D(G(z))) big]$ . Special Features of the DCGAN: . Explicitly uses convolutional layers in the discriminator and transposed-convolutional layers in the generator | Further the discriminator uses batch norm layers and LeakyReLU activations while the generator uses ReLU activations | The input is a latent vector drawn from a standard normal distribution and the output is a 3 x 32 x 32 RGB image | In this implementation, I also added in label smoothing | More details are in the paper | . Implementation Details: . I&#39;ve borrowed the majority of code for this from the wonderful Pytorch tutorial here but have made a couple of tweaks. The first of these is the structure of the generator and discriminator. Since the CIFAR-10 dataset has images of size 32 x 32, the output size of the generator and input size of the discriminator have to be changed. Secondly, I added in label smoothing to help with the stability of the training process. . Requirements . #collapse-hide #Author: Sairam Sundaresan #Version: 1.0 #Date May 1, 2020 # Preliminaries # WandB – Install the W&amp;B library %pip install wandb -q . . from __future__ import print_function import argparse import random # to set the python random seed %matplotlib inline import os import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation from IPython.display import HTML import torch import torch.nn as nn import torch.nn.functional as F import torch.nn.parallel import torch.backends.cudnn as cudnn import torch.utils.data import torchvision.utils as vutils import torch.optim as optim from torchvision import datasets, transforms # Ignore excessive warnings import logging logging.propagate = False logging.getLogger().setLevel(logging.ERROR) # Set random seed for reproducibility manualSeed = 42 random.seed(manualSeed) torch.manual_seed(manualSeed) # WandB – Import the wandb library import wandb wandb.login() wandb.init(project=&quot;dcgan&quot;) # Change the project name based on your W &amp; B account . Parameters of Interest . Note that the Pytorch tutorial referenced below is designed for the Celebrity faces dataset and produces 64 x 64 images. I&#39;ve tweaked the network architecture to produce 32 x 32 images as corresponding to the CIFAR-10 dataset. The parameters below reflect the same. . # Number of workers for dataloader workers = 2 # Batch size during training batch_size = 128 # Spatial size of training images. All images will be resized to this # size using a transformer. image_size = 32 # Number of channels in the training images. For color images this is 3 nc = 3 # Size of z latent vector (i.e. size of generator input) nz = 100 # Size of feature maps in generator ngf = 64 # Size of feature maps in discriminator ndf = 64 # Number of training epochs num_epochs = 30 # Learning rate for optimizers lr = 0.0002 # Beta1 hyperparam for Adam optimizers beta1 = 0.5 # Number of GPUs available. Use 0 for CPU mode. ngpu = 1 . Model Definition . Let&#39;s define a generator and discriminator first. Weight initialization is a key factor in being able to produce a decent GAN and as per the paper, the weights are drawn from a normal distribution with 0 mean and a standard-deviation of 0.02. Also note that unlike in the original pytorch tutorial, I&#39;ve removed one layer from the generator (at the end) and from the discriminator (at the beginning) to accomodate the CIFAR-10 dataset. . def weights_init(m): classname = m.__class__.__name__ if classname.find(&#39;Conv&#39;) != -1: nn.init.normal_(m.weight.data, 0.0, 0.02) elif classname.find(&#39;BatchNorm&#39;) != -1: nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0) . # Generator class Generator(nn.Module): def __init__(self, ngpu): super(Generator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 8), nn.ReLU(True), nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), nn.ConvTranspose2d( ngf * 2, nc, 4, 2, 1, bias=False), nn.Tanh() ) def forward(self, input): return self.main(input) . # Discriminator class Discriminator(nn.Module): def __init__(self, ngpu): super(Discriminator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False), nn.Sigmoid() ) def forward(self, input): return self.main(input) . Defining the Training Function . The training function first trains the discriminator and then the generator as shown below. Note that by setting the real label value to 0.9 and the fake label value to 0.1, I&#39;ve applied label smoothing which has been shown to improve the results produced by the GAN. . def train(args, gen, disc, device, dataloader, optimizerG, optimizerD, criterion, epoch, iters): gen.train() disc.train() img_list = [] fixed_noise = torch.randn(64, config.nz, 1, 1, device=device) # Establish convention for real and fake labels during training (with label smoothing) real_label = 0.9 fake_label = 0.1 for i, data in enumerate(dataloader, 0): #***** # Update Discriminator #***** ## Train with all-real batch disc.zero_grad() # Format batch real_cpu = data[0].to(device) b_size = real_cpu.size(0) label = torch.full((b_size,), real_label, device=device) # Forward pass real batch through D output = disc(real_cpu).view(-1) # Calculate loss on all-real batch errD_real = criterion(output, label) # Calculate gradients for D in backward pass errD_real.backward() D_x = output.mean().item() ## Train with all-fake batch # Generate batch of latent vectors noise = torch.randn(b_size, config.nz, 1, 1, device=device) # Generate fake image batch with G fake = gen(noise) label.fill_(fake_label) # Classify all fake batch with D output = disc(fake.detach()).view(-1) # Calculate D&#39;s loss on the all-fake batch errD_fake = criterion(output, label) # Calculate the gradients for this batch errD_fake.backward() D_G_z1 = output.mean().item() # Add the gradients from the all-real and all-fake batches errD = errD_real + errD_fake # Update D optimizerD.step() #***** # Update Generator #***** gen.zero_grad() label.fill_(real_label) # fake labels are real for generator cost # Since we just updated D, perform another forward pass of all-fake batch through D output = disc(fake).view(-1) # Calculate G&#39;s loss based on this output errG = criterion(output, label) # Calculate gradients for G errG.backward() D_G_z2 = output.mean().item() # Update G optimizerG.step() # Output training stats if i % 50 == 0: print(&#39;[%d/%d][%d/%d] tLoss_D: %.4f tLoss_G: %.4f tD(x): %.4f tD(G(z)): %.4f / %.4f&#39; % (epoch, args.epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2)) wandb.log({ &quot;Gen Loss&quot;: errG.item(), &quot;Disc Loss&quot;: errD.item()}) # Check how the generator is doing by saving G&#39;s output on fixed_noise if (iters % 500 == 0) or ((epoch == args.epochs-1) and (i == len(dataloader)-1)): with torch.no_grad(): fake = gen(fixed_noise).detach().cpu() img_list.append(wandb.Image(vutils.make_grid(fake, padding=2, normalize=True))) wandb.log({ &quot;Generated Images&quot;: img_list}) iters += 1 . Monitoring the Run . Once we have all the pieces in place, all we need to do is train the model and watch it learn. . #hide-collapse wandb.watch_called = False # WandB – Config is a variable that holds and saves hyperparameters and inputs config = wandb.config # Initialize config config.batch_size = batch_size config.epochs = num_epochs config.lr = lr config.beta1 = beta1 config.nz = nz config.no_cuda = False config.seed = manualSeed # random seed (default: 42) config.log_interval = 10 # how many batches to wait before logging training status def main(): use_cuda = not config.no_cuda and torch.cuda.is_available() device = torch.device(&quot;cuda&quot; if use_cuda else &quot;cpu&quot;) kwargs = {&#39;num_workers&#39;: 1, &#39;pin_memory&#39;: True} if use_cuda else {} # Set random seeds and deterministic pytorch for reproducibility random.seed(config.seed) # python random seed torch.manual_seed(config.seed) # pytorch random seed np.random.seed(config.seed) # numpy random seed torch.backends.cudnn.deterministic = True # Load the dataset transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=config.batch_size, shuffle=True, num_workers=workers) # Create the generator netG = Generator(ngpu).to(device) # Handle multi-gpu if desired if (device.type == &#39;cuda&#39;) and (ngpu &gt; 1): netG = nn.DataParallel(netG, list(range(ngpu))) # Apply the weights_init function to randomly initialize all weights # to mean=0, stdev=0.2. netG.apply(weights_init) # Create the Discriminator netD = Discriminator(ngpu).to(device) # Handle multi-gpu if desired if (device.type == &#39;cuda&#39;) and (ngpu &gt; 1): netD = nn.DataParallel(netD, list(range(ngpu))) # Apply the weights_init function to randomly initialize all weights # to mean=0, stdev=0.2. netD.apply(weights_init) # Initialize BCELoss function criterion = nn.BCELoss() # Setup Adam optimizers for both G and D optimizerD = optim.Adam(netD.parameters(), lr=config.lr, betas=(config.beta1, 0.999)) optimizerG = optim.Adam(netG.parameters(), lr=config.lr, betas=(config.beta1, 0.999)) # WandB – wandb.watch() automatically fetches all layer dimensions, gradients, model parameters and logs them automatically to your dashboard. # Using log=&quot;all&quot; log histograms of parameter values in addition to gradients wandb.watch(netG, log=&quot;all&quot;) wandb.watch(netD, log=&quot;all&quot;) iters = 0 for epoch in range(1, config.epochs + 1): train(config, netG, netD, device, trainloader, optimizerG, optimizerD, criterion, epoch, iters) # WandB – Save the model checkpoint. This automatically saves a file to the cloud and associates it with the current run. torch.save(netG.state_dict(), &quot;model.h5&quot;) wandb.save(&#39;model.h5&#39;) if __name__ == &#39;__main__&#39;: main() . Files already downloaded and verified [1/30][0/391] Loss_D: 1.5330 Loss_G: 2.0065 D(x): 0.4798 D(G(z)): 0.4893 / 0.1298 [1/30][50/391] Loss_D: 0.8715 Loss_G: 4.9647 D(x): 0.7946 D(G(z)): 0.3070 / 0.0045 [1/30][100/391] Loss_D: 0.7232 Loss_G: 3.8356 D(x): 0.8442 D(G(z)): 0.1959 / 0.0156 [1/30][150/391] Loss_D: 0.8965 Loss_G: 2.2287 D(x): 0.7690 D(G(z)): 0.2954 / 0.0948 [1/30][200/391] Loss_D: 0.9530 Loss_G: 1.8095 D(x): 0.6412 D(G(z)): 0.1987 / 0.1468 [1/30][250/391] Loss_D: 0.9138 Loss_G: 2.8588 D(x): 0.6709 D(G(z)): 0.1570 / 0.0513 [1/30][300/391] Loss_D: 0.8062 Loss_G: 2.6343 D(x): 0.7680 D(G(z)): 0.1848 / 0.0608 [1/30][350/391] Loss_D: 1.1042 Loss_G: 1.6227 D(x): 0.5420 D(G(z)): 0.1842 / 0.1997 [2/30][0/391] Loss_D: 1.0089 Loss_G: 1.7177 D(x): 0.5759 D(G(z)): 0.2028 / 0.1679 [2/30][50/391] Loss_D: 0.9720 Loss_G: 2.1761 D(x): 0.7768 D(G(z)): 0.3881 / 0.0989 [2/30][100/391] Loss_D: 0.9278 Loss_G: 2.0908 D(x): 0.7203 D(G(z)): 0.2952 / 0.1097 [2/30][150/391] Loss_D: 0.8625 Loss_G: 2.2617 D(x): 0.7206 D(G(z)): 0.1978 / 0.0911 [2/30][200/391] Loss_D: 1.1439 Loss_G: 2.3606 D(x): 0.6800 D(G(z)): 0.4345 / 0.0856 [2/30][250/391] Loss_D: 0.8159 Loss_G: 2.0152 D(x): 0.7410 D(G(z)): 0.1987 / 0.1190 [2/30][300/391] Loss_D: 0.8544 Loss_G: 2.1243 D(x): 0.6887 D(G(z)): 0.1167 / 0.1094 [2/30][350/391] Loss_D: 0.8503 Loss_G: 2.0028 D(x): 0.7512 D(G(z)): 0.2479 / 0.1256 [3/30][0/391] Loss_D: 1.0292 Loss_G: 2.3918 D(x): 0.8430 D(G(z)): 0.4425 / 0.0835 [3/30][50/391] Loss_D: 0.8971 Loss_G: 1.8851 D(x): 0.6703 D(G(z)): 0.1654 / 0.1446 [3/30][100/391] Loss_D: 1.0793 Loss_G: 1.3440 D(x): 0.5632 D(G(z)): 0.1374 / 0.2698 [3/30][150/391] Loss_D: 0.8603 Loss_G: 2.2827 D(x): 0.7910 D(G(z)): 0.2611 / 0.0984 [3/30][200/391] Loss_D: 0.9094 Loss_G: 2.0252 D(x): 0.7199 D(G(z)): 0.2683 / 0.1289 [3/30][250/391] Loss_D: 0.8966 Loss_G: 2.1354 D(x): 0.7764 D(G(z)): 0.2912 / 0.1119 [3/30][300/391] Loss_D: 0.8312 Loss_G: 1.7893 D(x): 0.7383 D(G(z)): 0.2004 / 0.1578 [3/30][350/391] Loss_D: 1.0215 Loss_G: 1.3776 D(x): 0.6032 D(G(z)): 0.2551 / 0.2479 [4/30][0/391] Loss_D: 0.9510 Loss_G: 2.0825 D(x): 0.7698 D(G(z)): 0.3555 / 0.1153 [4/30][50/391] Loss_D: 0.8927 Loss_G: 1.9853 D(x): 0.7803 D(G(z)): 0.3192 / 0.1255 [4/30][100/391] Loss_D: 0.8228 Loss_G: 1.5415 D(x): 0.7643 D(G(z)): 0.2403 / 0.2008 [4/30][150/391] Loss_D: 0.8701 Loss_G: 1.8234 D(x): 0.7514 D(G(z)): 0.2680 / 0.1555 [4/30][200/391] Loss_D: 0.8611 Loss_G: 1.5613 D(x): 0.6997 D(G(z)): 0.1834 / 0.2065 [4/30][250/391] Loss_D: 0.7854 Loss_G: 2.0144 D(x): 0.8383 D(G(z)): 0.2203 / 0.1215 [4/30][300/391] Loss_D: 0.7546 Loss_G: 2.0268 D(x): 0.7897 D(G(z)): 0.1478 / 0.1218 [4/30][350/391] Loss_D: 0.9487 Loss_G: 1.3846 D(x): 0.6133 D(G(z)): 0.1861 / 0.2547 [5/30][0/391] Loss_D: 1.0132 Loss_G: 0.7042 D(x): 0.5492 D(G(z)): 0.1104 / 0.5286 [5/30][50/391] Loss_D: 0.9648 Loss_G: 1.9506 D(x): 0.7825 D(G(z)): 0.3778 / 0.1327 [5/30][100/391] Loss_D: 0.8504 Loss_G: 1.5864 D(x): 0.7236 D(G(z)): 0.2211 / 0.1928 [5/30][150/391] Loss_D: 0.9893 Loss_G: 1.4641 D(x): 0.7339 D(G(z)): 0.3483 / 0.2283 [5/30][200/391] Loss_D: 0.9685 Loss_G: 1.4706 D(x): 0.6254 D(G(z)): 0.2019 / 0.2398 [5/30][250/391] Loss_D: 0.9474 Loss_G: 1.9077 D(x): 0.7109 D(G(z)): 0.3222 / 0.1389 [5/30][300/391] Loss_D: 1.0227 Loss_G: 1.4165 D(x): 0.6255 D(G(z)): 0.2816 / 0.2360 [5/30][350/391] Loss_D: 1.1103 Loss_G: 1.0159 D(x): 0.5182 D(G(z)): 0.1977 / 0.3701 [6/30][0/391] Loss_D: 0.8827 Loss_G: 1.6163 D(x): 0.6925 D(G(z)): 0.2418 / 0.1865 [6/30][50/391] Loss_D: 1.1054 Loss_G: 2.2958 D(x): 0.8291 D(G(z)): 0.4917 / 0.0939 [6/30][100/391] Loss_D: 0.9841 Loss_G: 1.4822 D(x): 0.6658 D(G(z)): 0.3004 / 0.2176 [6/30][150/391] Loss_D: 0.9598 Loss_G: 1.5836 D(x): 0.7312 D(G(z)): 0.3420 / 0.1988 [6/30][200/391] Loss_D: 1.0637 Loss_G: 1.3479 D(x): 0.6263 D(G(z)): 0.3189 / 0.2635 [6/30][250/391] Loss_D: 1.0678 Loss_G: 1.8553 D(x): 0.7680 D(G(z)): 0.4433 / 0.1513 [6/30][300/391] Loss_D: 1.0556 Loss_G: 1.8421 D(x): 0.7322 D(G(z)): 0.4143 / 0.1496 [6/30][350/391] Loss_D: 1.0985 Loss_G: 1.9509 D(x): 0.7936 D(G(z)): 0.4807 / 0.1308 [7/30][0/391] Loss_D: 0.9637 Loss_G: 1.3168 D(x): 0.6677 D(G(z)): 0.2981 / 0.2594 [7/30][50/391] Loss_D: 1.0530 Loss_G: 1.1391 D(x): 0.5883 D(G(z)): 0.2867 / 0.3141 [7/30][100/391] Loss_D: 0.9574 Loss_G: 1.4361 D(x): 0.6782 D(G(z)): 0.3015 / 0.2262 [7/30][150/391] Loss_D: 1.0266 Loss_G: 1.4384 D(x): 0.6787 D(G(z)): 0.3633 / 0.2260 [7/30][200/391] Loss_D: 1.2161 Loss_G: 2.5106 D(x): 0.7031 D(G(z)): 0.5100 / 0.0720 [7/30][250/391] Loss_D: 0.9846 Loss_G: 1.5405 D(x): 0.7217 D(G(z)): 0.3592 / 0.2093 [7/30][300/391] Loss_D: 1.0500 Loss_G: 1.0985 D(x): 0.5870 D(G(z)): 0.2566 / 0.3348 [7/30][350/391] Loss_D: 0.9962 Loss_G: 1.3818 D(x): 0.6454 D(G(z)): 0.3064 / 0.2391 [8/30][0/391] Loss_D: 1.5409 Loss_G: 0.8797 D(x): 0.3085 D(G(z)): 0.1152 / 0.4338 [8/30][50/391] Loss_D: 1.1932 Loss_G: 1.0164 D(x): 0.4724 D(G(z)): 0.1956 / 0.3694 [8/30][100/391] Loss_D: 1.1094 Loss_G: 1.9686 D(x): 0.8014 D(G(z)): 0.4915 / 0.1341 [8/30][150/391] Loss_D: 1.0402 Loss_G: 1.0891 D(x): 0.5796 D(G(z)): 0.2517 / 0.3365 [8/30][200/391] Loss_D: 1.0166 Loss_G: 1.1981 D(x): 0.6393 D(G(z)): 0.3064 / 0.2972 [8/30][250/391] Loss_D: 1.3140 Loss_G: 2.1779 D(x): 0.7989 D(G(z)): 0.5972 / 0.1066 [8/30][300/391] Loss_D: 1.0808 Loss_G: 1.3231 D(x): 0.6657 D(G(z)): 0.3929 / 0.2597 [8/30][350/391] Loss_D: 0.9214 Loss_G: 1.6856 D(x): 0.7807 D(G(z)): 0.3585 / 0.1708 [9/30][0/391] Loss_D: 1.0025 Loss_G: 1.4469 D(x): 0.6824 D(G(z)): 0.3375 / 0.2321 [9/30][50/391] Loss_D: 1.3056 Loss_G: 1.2976 D(x): 0.6130 D(G(z)): 0.5060 / 0.2650 [9/30][100/391] Loss_D: 0.9936 Loss_G: 1.3405 D(x): 0.5984 D(G(z)): 0.2396 / 0.2562 [9/30][150/391] Loss_D: 1.2408 Loss_G: 1.0987 D(x): 0.4876 D(G(z)): 0.3149 / 0.3332 [9/30][200/391] Loss_D: 0.9973 Loss_G: 1.4097 D(x): 0.6510 D(G(z)): 0.3216 / 0.2331 [9/30][250/391] Loss_D: 1.0263 Loss_G: 1.2741 D(x): 0.6090 D(G(z)): 0.2867 / 0.2714 [9/30][300/391] Loss_D: 1.1614 Loss_G: 2.1137 D(x): 0.8225 D(G(z)): 0.5258 / 0.1121 [9/30][350/391] Loss_D: 1.0194 Loss_G: 1.0657 D(x): 0.6055 D(G(z)): 0.2866 / 0.3395 [10/30][0/391] Loss_D: 1.0612 Loss_G: 1.2795 D(x): 0.6027 D(G(z)): 0.3106 / 0.2702 [10/30][50/391] Loss_D: 1.1699 Loss_G: 1.0217 D(x): 0.5101 D(G(z)): 0.2744 / 0.3589 [10/30][100/391] Loss_D: 1.0647 Loss_G: 1.7456 D(x): 0.7280 D(G(z)): 0.4372 / 0.1606 [10/30][150/391] Loss_D: 1.2592 Loss_G: 0.8300 D(x): 0.4822 D(G(z)): 0.3269 / 0.4461 [10/30][200/391] Loss_D: 0.9582 Loss_G: 1.8927 D(x): 0.7721 D(G(z)): 0.3860 / 0.1358 [10/30][250/391] Loss_D: 1.0677 Loss_G: 1.4276 D(x): 0.6790 D(G(z)): 0.4053 / 0.2250 [10/30][300/391] Loss_D: 1.1234 Loss_G: 1.2369 D(x): 0.5594 D(G(z)): 0.3270 / 0.2799 [10/30][350/391] Loss_D: 1.0698 Loss_G: 1.6208 D(x): 0.7850 D(G(z)): 0.4724 / 0.1830 [11/30][0/391] Loss_D: 0.9760 Loss_G: 1.2819 D(x): 0.6136 D(G(z)): 0.2478 / 0.2686 [11/30][50/391] Loss_D: 0.9170 Loss_G: 1.3954 D(x): 0.6942 D(G(z)): 0.2872 / 0.2354 [11/30][100/391] Loss_D: 0.9664 Loss_G: 1.2902 D(x): 0.6745 D(G(z)): 0.3175 / 0.2676 [11/30][150/391] Loss_D: 1.0373 Loss_G: 1.1947 D(x): 0.6357 D(G(z)): 0.3377 / 0.2964 [11/30][200/391] Loss_D: 1.0634 Loss_G: 1.6096 D(x): 0.6739 D(G(z)): 0.3937 / 0.1897 [11/30][250/391] Loss_D: 0.9452 Loss_G: 1.7660 D(x): 0.7718 D(G(z)): 0.3655 / 0.1644 [11/30][300/391] Loss_D: 1.0072 Loss_G: 1.3114 D(x): 0.5903 D(G(z)): 0.2331 / 0.2578 [11/30][350/391] Loss_D: 1.8891 Loss_G: 0.3712 D(x): 0.2055 D(G(z)): 0.0591 / 0.9124 [12/30][0/391] Loss_D: 0.9950 Loss_G: 1.4577 D(x): 0.6965 D(G(z)): 0.3567 / 0.2259 [12/30][50/391] Loss_D: 1.0831 Loss_G: 1.0768 D(x): 0.5093 D(G(z)): 0.1850 / 0.3359 [12/30][100/391] Loss_D: 0.9436 Loss_G: 1.3149 D(x): 0.5983 D(G(z)): 0.1752 / 0.2583 [12/30][150/391] Loss_D: 0.9256 Loss_G: 1.5862 D(x): 0.7769 D(G(z)): 0.3604 / 0.1917 [12/30][200/391] Loss_D: 1.5656 Loss_G: 1.7253 D(x): 0.8284 D(G(z)): 0.6978 / 0.1704 [12/30][250/391] Loss_D: 1.1823 Loss_G: 1.3516 D(x): 0.6949 D(G(z)): 0.4968 / 0.2446 [12/30][300/391] Loss_D: 1.0467 Loss_G: 1.4240 D(x): 0.7208 D(G(z)): 0.4215 / 0.2271 [12/30][350/391] Loss_D: 1.0150 Loss_G: 1.5902 D(x): 0.7788 D(G(z)): 0.4328 / 0.1881 [13/30][0/391] Loss_D: 1.0308 Loss_G: 1.4838 D(x): 0.7335 D(G(z)): 0.4172 / 0.2118 [13/30][50/391] Loss_D: 1.0585 Loss_G: 1.1484 D(x): 0.5756 D(G(z)): 0.2751 / 0.3161 [13/30][100/391] Loss_D: 1.0553 Loss_G: 1.5469 D(x): 0.6518 D(G(z)): 0.3704 / 0.2003 [13/30][150/391] Loss_D: 1.0285 Loss_G: 1.3155 D(x): 0.6715 D(G(z)): 0.3604 / 0.2569 [13/30][200/391] Loss_D: 1.0419 Loss_G: 1.1947 D(x): 0.6208 D(G(z)): 0.3158 / 0.3006 [13/30][250/391] Loss_D: 1.3148 Loss_G: 0.3872 D(x): 0.4021 D(G(z)): 0.2104 / 0.7859 [13/30][300/391] Loss_D: 0.9487 Loss_G: 1.4255 D(x): 0.6913 D(G(z)): 0.3121 / 0.2279 [13/30][350/391] Loss_D: 1.1076 Loss_G: 1.0830 D(x): 0.5353 D(G(z)): 0.2674 / 0.3348 [14/30][0/391] Loss_D: 1.0612 Loss_G: 1.0598 D(x): 0.5697 D(G(z)): 0.2760 / 0.3424 [14/30][50/391] Loss_D: 1.0218 Loss_G: 1.9751 D(x): 0.7966 D(G(z)): 0.4417 / 0.1271 [14/30][100/391] Loss_D: 1.3000 Loss_G: 0.5739 D(x): 0.3979 D(G(z)): 0.1513 / 0.6069 [14/30][150/391] Loss_D: 0.9931 Loss_G: 1.8432 D(x): 0.7729 D(G(z)): 0.4132 / 0.1446 [14/30][200/391] Loss_D: 1.1232 Loss_G: 1.1526 D(x): 0.5320 D(G(z)): 0.2618 / 0.3120 [14/30][250/391] Loss_D: 1.1957 Loss_G: 1.4379 D(x): 0.6904 D(G(z)): 0.4878 / 0.2271 [14/30][300/391] Loss_D: 1.0007 Loss_G: 1.2963 D(x): 0.6535 D(G(z)): 0.3146 / 0.2652 [14/30][350/391] Loss_D: 1.0849 Loss_G: 1.0480 D(x): 0.5315 D(G(z)): 0.2361 / 0.3496 [15/30][0/391] Loss_D: 1.0305 Loss_G: 1.3248 D(x): 0.6690 D(G(z)): 0.3635 / 0.2535 [15/30][50/391] Loss_D: 1.3088 Loss_G: 0.5679 D(x): 0.3888 D(G(z)): 0.1352 / 0.6100 [15/30][100/391] Loss_D: 1.0869 Loss_G: 1.4303 D(x): 0.6516 D(G(z)): 0.3942 / 0.2296 [15/30][150/391] Loss_D: 1.1082 Loss_G: 0.9655 D(x): 0.5181 D(G(z)): 0.2441 / 0.3799 [15/30][200/391] Loss_D: 1.1288 Loss_G: 1.5768 D(x): 0.7549 D(G(z)): 0.4903 / 0.1930 [15/30][250/391] Loss_D: 1.6270 Loss_G: 0.8472 D(x): 0.4068 D(G(z)): 0.4135 / 0.4905 [15/30][300/391] Loss_D: 1.2084 Loss_G: 1.0170 D(x): 0.5059 D(G(z)): 0.3147 / 0.3601 [15/30][350/391] Loss_D: 1.0819 Loss_G: 1.1340 D(x): 0.5780 D(G(z)): 0.3061 / 0.3156 [16/30][0/391] Loss_D: 1.1331 Loss_G: 1.4749 D(x): 0.6504 D(G(z)): 0.4252 / 0.2184 [16/30][50/391] Loss_D: 1.0175 Loss_G: 1.0656 D(x): 0.5682 D(G(z)): 0.2266 / 0.3423 [16/30][100/391] Loss_D: 1.0190 Loss_G: 1.6862 D(x): 0.7375 D(G(z)): 0.4017 / 0.1736 [16/30][150/391] Loss_D: 1.0497 Loss_G: 1.1752 D(x): 0.5856 D(G(z)): 0.2815 / 0.3090 [16/30][200/391] Loss_D: 1.6160 Loss_G: 0.3481 D(x): 0.2845 D(G(z)): 0.1125 / 0.8798 [16/30][250/391] Loss_D: 1.0176 Loss_G: 1.2730 D(x): 0.6313 D(G(z)): 0.3164 / 0.2637 [16/30][300/391] Loss_D: 1.0558 Loss_G: 1.0953 D(x): 0.5947 D(G(z)): 0.3055 / 0.3273 [16/30][350/391] Loss_D: 1.1903 Loss_G: 1.2424 D(x): 0.6171 D(G(z)): 0.4396 / 0.2823 [17/30][0/391] Loss_D: 1.0670 Loss_G: 1.3721 D(x): 0.6998 D(G(z)): 0.4217 / 0.2446 [17/30][50/391] Loss_D: 0.9566 Loss_G: 1.2191 D(x): 0.6072 D(G(z)): 0.2059 / 0.2872 [17/30][100/391] Loss_D: 1.1072 Loss_G: 1.0355 D(x): 0.5674 D(G(z)): 0.3136 / 0.3509 [17/30][150/391] Loss_D: 1.5534 Loss_G: 1.6169 D(x): 0.7082 D(G(z)): 0.6704 / 0.1896 [17/30][200/391] Loss_D: 1.0195 Loss_G: 1.1475 D(x): 0.6366 D(G(z)): 0.3214 / 0.3176 [17/30][250/391] Loss_D: 1.0278 Loss_G: 1.2424 D(x): 0.6161 D(G(z)): 0.3034 / 0.2853 [17/30][300/391] Loss_D: 0.9902 Loss_G: 1.4736 D(x): 0.7227 D(G(z)): 0.3792 / 0.2160 [17/30][350/391] Loss_D: 1.1040 Loss_G: 0.9200 D(x): 0.5122 D(G(z)): 0.2319 / 0.4013 [18/30][0/391] Loss_D: 1.0960 Loss_G: 1.0392 D(x): 0.5725 D(G(z)): 0.3085 / 0.3482 [18/30][50/391] Loss_D: 0.9788 Loss_G: 1.5755 D(x): 0.7268 D(G(z)): 0.3811 / 0.1882 [18/30][100/391] Loss_D: 0.8997 Loss_G: 1.5019 D(x): 0.7111 D(G(z)): 0.2917 / 0.2094 [18/30][150/391] Loss_D: 1.2186 Loss_G: 1.7073 D(x): 0.7189 D(G(z)): 0.5183 / 0.1728 [18/30][200/391] Loss_D: 1.3071 Loss_G: 0.8504 D(x): 0.4213 D(G(z)): 0.2571 / 0.4327 [18/30][250/391] Loss_D: 1.1458 Loss_G: 0.8859 D(x): 0.5040 D(G(z)): 0.2565 / 0.4144 [18/30][300/391] Loss_D: 1.0417 Loss_G: 0.9717 D(x): 0.5995 D(G(z)): 0.3028 / 0.3742 [18/30][350/391] Loss_D: 1.0133 Loss_G: 1.3466 D(x): 0.7132 D(G(z)): 0.3863 / 0.2500 [19/30][0/391] Loss_D: 1.3726 Loss_G: 2.0758 D(x): 0.7981 D(G(z)): 0.6310 / 0.1142 [19/30][50/391] Loss_D: 0.9745 Loss_G: 1.5558 D(x): 0.7433 D(G(z)): 0.3790 / 0.1957 [19/30][100/391] Loss_D: 1.0528 Loss_G: 0.9203 D(x): 0.5669 D(G(z)): 0.2604 / 0.4082 [19/30][150/391] Loss_D: 1.5363 Loss_G: 0.8982 D(x): 0.4001 D(G(z)): 0.4051 / 0.4157 [19/30][200/391] Loss_D: 1.1483 Loss_G: 0.7528 D(x): 0.4958 D(G(z)): 0.2292 / 0.4856 [19/30][250/391] Loss_D: 1.0381 Loss_G: 1.3302 D(x): 0.6641 D(G(z)): 0.3737 / 0.2531 [19/30][300/391] Loss_D: 1.1083 Loss_G: 0.9069 D(x): 0.5452 D(G(z)): 0.2756 / 0.4079 [19/30][350/391] Loss_D: 1.1544 Loss_G: 1.2182 D(x): 0.6001 D(G(z)): 0.3961 / 0.2831 [20/30][0/391] Loss_D: 1.0190 Loss_G: 1.2576 D(x): 0.6303 D(G(z)): 0.3136 / 0.2755 [20/30][50/391] Loss_D: 1.0354 Loss_G: 1.5062 D(x): 0.6917 D(G(z)): 0.3959 / 0.2054 [20/30][100/391] Loss_D: 1.2535 Loss_G: 0.7139 D(x): 0.4359 D(G(z)): 0.2179 / 0.5107 [20/30][150/391] Loss_D: 1.0953 Loss_G: 1.2183 D(x): 0.6362 D(G(z)): 0.3834 / 0.2883 [20/30][200/391] Loss_D: 1.0546 Loss_G: 1.3230 D(x): 0.7128 D(G(z)): 0.4188 / 0.2540 [20/30][250/391] Loss_D: 1.0512 Loss_G: 1.1199 D(x): 0.5592 D(G(z)): 0.2516 / 0.3177 [20/30][300/391] Loss_D: 1.0122 Loss_G: 1.3715 D(x): 0.7465 D(G(z)): 0.4099 / 0.2427 [20/30][350/391] Loss_D: 1.0544 Loss_G: 1.3656 D(x): 0.6835 D(G(z)): 0.3958 / 0.2388 [21/30][0/391] Loss_D: 1.0685 Loss_G: 1.8458 D(x): 0.7763 D(G(z)): 0.4662 / 0.1463 [21/30][50/391] Loss_D: 1.0744 Loss_G: 1.2440 D(x): 0.6764 D(G(z)): 0.4115 / 0.2804 [21/30][100/391] Loss_D: 1.0559 Loss_G: 1.3944 D(x): 0.6855 D(G(z)): 0.4008 / 0.2356 [21/30][150/391] Loss_D: 1.1035 Loss_G: 1.1176 D(x): 0.5713 D(G(z)): 0.3236 / 0.3213 [21/30][200/391] Loss_D: 1.0946 Loss_G: 1.0384 D(x): 0.5235 D(G(z)): 0.2393 / 0.3490 [21/30][250/391] Loss_D: 1.5409 Loss_G: 2.3917 D(x): 0.8745 D(G(z)): 0.6994 / 0.0843 [21/30][300/391] Loss_D: 1.1315 Loss_G: 0.9393 D(x): 0.5008 D(G(z)): 0.2352 / 0.3872 [21/30][350/391] Loss_D: 1.0280 Loss_G: 1.2758 D(x): 0.6088 D(G(z)): 0.2952 / 0.2737 [22/30][0/391] Loss_D: 1.1280 Loss_G: 1.4459 D(x): 0.6356 D(G(z)): 0.3952 / 0.2246 [22/30][50/391] Loss_D: 1.1620 Loss_G: 0.9946 D(x): 0.5696 D(G(z)): 0.3604 / 0.3703 [22/30][100/391] Loss_D: 0.9770 Loss_G: 1.4543 D(x): 0.6995 D(G(z)): 0.3471 / 0.2229 [22/30][150/391] Loss_D: 1.0864 Loss_G: 1.3690 D(x): 0.6791 D(G(z)): 0.4188 / 0.2381 [22/30][200/391] Loss_D: 1.2507 Loss_G: 0.7848 D(x): 0.4238 D(G(z)): 0.1824 / 0.4673 [22/30][250/391] Loss_D: 1.4277 Loss_G: 1.1070 D(x): 0.6578 D(G(z)): 0.5827 / 0.3285 [22/30][300/391] Loss_D: 1.1703 Loss_G: 2.1975 D(x): 0.8293 D(G(z)): 0.5382 / 0.1004 [22/30][350/391] Loss_D: 1.4329 Loss_G: 3.3520 D(x): 0.7813 D(G(z)): 0.6275 / 0.0304 [23/30][0/391] Loss_D: 1.1567 Loss_G: 1.1079 D(x): 0.5522 D(G(z)): 0.3503 / 0.3193 [23/30][50/391] Loss_D: 1.1654 Loss_G: 1.4560 D(x): 0.7406 D(G(z)): 0.5053 / 0.2225 [23/30][100/391] Loss_D: 1.0584 Loss_G: 1.2907 D(x): 0.6146 D(G(z)): 0.3307 / 0.2639 [23/30][150/391] Loss_D: 1.1503 Loss_G: 0.9929 D(x): 0.4971 D(G(z)): 0.2458 / 0.3717 [23/30][200/391] Loss_D: 1.0898 Loss_G: 1.6250 D(x): 0.7449 D(G(z)): 0.4576 / 0.1812 [23/30][250/391] Loss_D: 1.2575 Loss_G: 0.6943 D(x): 0.4095 D(G(z)): 0.1767 / 0.5149 [23/30][300/391] Loss_D: 1.0266 Loss_G: 1.2971 D(x): 0.6639 D(G(z)): 0.3590 / 0.2624 [23/30][350/391] Loss_D: 1.0735 Loss_G: 1.2237 D(x): 0.5776 D(G(z)): 0.3023 / 0.2847 [24/30][0/391] Loss_D: 1.1004 Loss_G: 1.1137 D(x): 0.5817 D(G(z)): 0.3335 / 0.3219 [24/30][50/391] Loss_D: 2.0557 Loss_G: 0.4472 D(x): 0.1792 D(G(z)): 0.1065 / 0.7225 [24/30][100/391] Loss_D: 1.0702 Loss_G: 1.1436 D(x): 0.6057 D(G(z)): 0.3222 / 0.3111 [24/30][150/391] Loss_D: 0.9627 Loss_G: 1.4126 D(x): 0.6567 D(G(z)): 0.2861 / 0.2377 [24/30][200/391] Loss_D: 0.9807 Loss_G: 1.4336 D(x): 0.6841 D(G(z)): 0.3353 / 0.2267 [24/30][250/391] Loss_D: 1.0433 Loss_G: 1.0720 D(x): 0.6218 D(G(z)): 0.3217 / 0.3369 [24/30][300/391] Loss_D: 1.0439 Loss_G: 1.4228 D(x): 0.6963 D(G(z)): 0.4011 / 0.2277 [24/30][350/391] Loss_D: 1.0372 Loss_G: 0.9824 D(x): 0.5723 D(G(z)): 0.2579 / 0.3742 [25/30][0/391] Loss_D: 1.1856 Loss_G: 1.1063 D(x): 0.6386 D(G(z)): 0.4578 / 0.3232 [25/30][50/391] Loss_D: 1.0876 Loss_G: 1.1943 D(x): 0.6299 D(G(z)): 0.3757 / 0.2938 [25/30][100/391] Loss_D: 1.1746 Loss_G: 1.2097 D(x): 0.4657 D(G(z)): 0.1787 / 0.3032 [25/30][150/391] Loss_D: 1.0051 Loss_G: 1.1847 D(x): 0.6147 D(G(z)): 0.2745 / 0.3018 [25/30][200/391] Loss_D: 1.2852 Loss_G: 0.9885 D(x): 0.4231 D(G(z)): 0.2150 / 0.3709 [25/30][250/391] Loss_D: 1.0958 Loss_G: 1.7921 D(x): 0.6723 D(G(z)): 0.4034 / 0.1603 [25/30][300/391] Loss_D: 1.0115 Loss_G: 1.4030 D(x): 0.7162 D(G(z)): 0.3908 / 0.2361 [25/30][350/391] Loss_D: 0.9268 Loss_G: 1.5395 D(x): 0.7064 D(G(z)): 0.3069 / 0.2030 [26/30][0/391] Loss_D: 1.0486 Loss_G: 0.9544 D(x): 0.5362 D(G(z)): 0.1967 / 0.3835 [26/30][50/391] Loss_D: 1.0524 Loss_G: 1.1044 D(x): 0.6183 D(G(z)): 0.3383 / 0.3202 [26/30][100/391] Loss_D: 1.4844 Loss_G: 2.2050 D(x): 0.8661 D(G(z)): 0.6792 / 0.1010 [26/30][150/391] Loss_D: 1.0594 Loss_G: 1.3569 D(x): 0.6854 D(G(z)): 0.4001 / 0.2443 [26/30][200/391] Loss_D: 0.9221 Loss_G: 1.6898 D(x): 0.7913 D(G(z)): 0.3668 / 0.1734 [26/30][250/391] Loss_D: 1.0627 Loss_G: 1.1584 D(x): 0.5780 D(G(z)): 0.2944 / 0.3073 [26/30][300/391] Loss_D: 1.0118 Loss_G: 0.7999 D(x): 0.5995 D(G(z)): 0.2603 / 0.4587 [26/30][350/391] Loss_D: 1.0222 Loss_G: 1.3166 D(x): 0.7171 D(G(z)): 0.4007 / 0.2560 [27/30][0/391] Loss_D: 1.0684 Loss_G: 1.5186 D(x): 0.7177 D(G(z)): 0.4342 / 0.2081 [27/30][50/391] Loss_D: 0.9575 Loss_G: 1.1518 D(x): 0.6267 D(G(z)): 0.2523 / 0.3079 [27/30][100/391] Loss_D: 1.2484 Loss_G: 1.5434 D(x): 0.7478 D(G(z)): 0.5502 / 0.2036 [27/30][150/391] Loss_D: 1.0930 Loss_G: 1.9262 D(x): 0.8013 D(G(z)): 0.4914 / 0.1325 [27/30][200/391] Loss_D: 0.9854 Loss_G: 1.3368 D(x): 0.6414 D(G(z)): 0.2801 / 0.2528 [27/30][250/391] Loss_D: 1.1274 Loss_G: 0.8990 D(x): 0.5336 D(G(z)): 0.2889 / 0.4060 [27/30][300/391] Loss_D: 1.1298 Loss_G: 1.3775 D(x): 0.7048 D(G(z)): 0.4627 / 0.2430 [27/30][350/391] Loss_D: 1.1789 Loss_G: 1.0439 D(x): 0.5592 D(G(z)): 0.3604 / 0.3515 [28/30][0/391] Loss_D: 1.2327 Loss_G: 0.4293 D(x): 0.4363 D(G(z)): 0.1641 / 0.7413 [28/30][50/391] Loss_D: 1.0969 Loss_G: 1.7167 D(x): 0.7227 D(G(z)): 0.4571 / 0.1647 [28/30][100/391] Loss_D: 1.2208 Loss_G: 1.8356 D(x): 0.7656 D(G(z)): 0.5422 / 0.1485 [28/30][150/391] Loss_D: 0.9899 Loss_G: 1.2459 D(x): 0.6296 D(G(z)): 0.2899 / 0.2780 [28/30][200/391] Loss_D: 1.1061 Loss_G: 1.2617 D(x): 0.7131 D(G(z)): 0.4436 / 0.2766 [28/30][250/391] Loss_D: 1.0123 Loss_G: 1.5527 D(x): 0.7465 D(G(z)): 0.4134 / 0.1986 [28/30][300/391] Loss_D: 1.0897 Loss_G: 1.3486 D(x): 0.7490 D(G(z)): 0.4658 / 0.2474 [28/30][350/391] Loss_D: 1.2090 Loss_G: 1.5242 D(x): 0.7159 D(G(z)): 0.5127 / 0.2085 [29/30][0/391] Loss_D: 1.0897 Loss_G: 1.8636 D(x): 0.8098 D(G(z)): 0.4832 / 0.1450 [29/30][50/391] Loss_D: 0.9900 Loss_G: 1.1591 D(x): 0.6199 D(G(z)): 0.2589 / 0.3121 [29/30][100/391] Loss_D: 0.9487 Loss_G: 1.1538 D(x): 0.6007 D(G(z)): 0.1828 / 0.3089 [29/30][150/391] Loss_D: 1.1695 Loss_G: 0.9479 D(x): 0.5475 D(G(z)): 0.3373 / 0.3911 [29/30][200/391] Loss_D: 0.9855 Loss_G: 1.2028 D(x): 0.6059 D(G(z)): 0.2403 / 0.2930 [29/30][250/391] Loss_D: 0.9087 Loss_G: 1.2191 D(x): 0.6626 D(G(z)): 0.2366 / 0.2860 [29/30][300/391] Loss_D: 1.1042 Loss_G: 1.9367 D(x): 0.8288 D(G(z)): 0.5125 / 0.1322 [29/30][350/391] Loss_D: 1.1310 Loss_G: 0.9877 D(x): 0.5493 D(G(z)): 0.3014 / 0.3761 [30/30][0/391] Loss_D: 0.9217 Loss_G: 1.0368 D(x): 0.6388 D(G(z)): 0.2206 / 0.3460 [30/30][50/391] Loss_D: 1.3077 Loss_G: 2.0492 D(x): 0.7052 D(G(z)): 0.5517 / 0.1267 [30/30][100/391] Loss_D: 1.0263 Loss_G: 1.0027 D(x): 0.5979 D(G(z)): 0.2794 / 0.3663 [30/30][150/391] Loss_D: 1.3014 Loss_G: 0.6843 D(x): 0.3940 D(G(z)): 0.1646 / 0.5306 [30/30][200/391] Loss_D: 1.3951 Loss_G: 0.9962 D(x): 0.6439 D(G(z)): 0.5578 / 0.3756 [30/30][250/391] Loss_D: 1.0112 Loss_G: 1.5962 D(x): 0.7862 D(G(z)): 0.4260 / 0.1940 [30/30][300/391] Loss_D: 1.1134 Loss_G: 1.4656 D(x): 0.6945 D(G(z)): 0.4396 / 0.2217 [30/30][350/391] Loss_D: 1.0365 Loss_G: 1.6098 D(x): 0.7294 D(G(z)): 0.4134 / 0.1910 . Loss Curve and Results . For a few lines of code, the GAN produces pretty decent images in 30 epochs as you can see below. . Loss . Images . References . DCGAN Pytorch Tutorial: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html |",
            "url": "https://ssundar6087.github.io/vision-and-words/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html",
            "relUrl": "/python/computer%20vision/deep%20learning/gans/pytorch/2020/05/01/DCGAN-CIFAR10.html",
            "date": " • May 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ssundar6087.github.io/vision-and-words/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey there! I’m a Computer Vision and Machine Learning research engineer by day. I’m very interested in areas such as Object tracking, Computational Photography, 3D Reconstruction and Interaction (AR/VR). In my free time you can find me photographing landscapes or sitting my by computer trying to figure out Kaggle :) . .",
          "url": "https://ssundar6087.github.io/vision-and-words/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ssundar6087.github.io/vision-and-words/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}